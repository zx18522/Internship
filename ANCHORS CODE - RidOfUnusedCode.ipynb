{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import collections\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import string\n",
    "from io import open\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chicken-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import io\n",
    "\n",
    "\n",
    "\"\"\" ~~~~~ Used in class AnchorBaseBeam(object): ~~~~~ \"\"\"\n",
    "class AnchorExplanation:\n",
    "    \"\"\"Object returned by explainers\"\"\"\n",
    "    def __init__(self, type_, exp_map, as_html):\n",
    "        self.type = type_\n",
    "        self.exp_map = exp_map\n",
    "        self.as_html_fn = as_html\n",
    "    \n",
    "    \"\"\" Not in the main code, just when printting the rages of the anchor's features\"\"\"\n",
    "    def names(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns a list of the names of the anchor conditions.\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor until a certain index.\n",
    "            For example, if the anchor is (A=1,B=2,C=2) and partial_index=1,\n",
    "            this will return [\"A=1\", \"B=2\"]\n",
    "        \"\"\"\n",
    "        names = self.exp_map['names']\n",
    "        if partial_index is not None:\n",
    "            names = names[:partial_index + 1]\n",
    "        return names\n",
    "    \n",
    "    \"\"\" Not in the main code, returns the index of the anchor's features used\"\"\"\n",
    "    def features(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns a list of the features used in the anchor conditions.\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor until a certain index.\n",
    "            For example, if the anchor uses features (1, 2, 3) and\n",
    "            partial_index=1, this will return [1, 2]\n",
    "        \"\"\"\n",
    "        features = self.exp_map['feature']\n",
    "        if partial_index is not None:\n",
    "            features = features[:partial_index + 1]\n",
    "        return features\n",
    "    \n",
    "    \"\"\" Not in the main code, returns precision of the anchor's\"\"\"\n",
    "    def precision(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns the anchor precision (a float)\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor precision until a\n",
    "            certain index. For example, if the anchor has precisions\n",
    "            [0.1, 0.5, 0.95] and partial_index=1, this will return 0.5\n",
    "        \"\"\"\n",
    "        precision = self.exp_map['precision']\n",
    "        if len(precision) == 0:\n",
    "            return self.exp_map['all_precision']\n",
    "        if partial_index is not None:\n",
    "            return precision[partial_index]\n",
    "        else:\n",
    "            return precision[-1]\n",
    "\n",
    "    \"\"\" Not in the main code, returns coverage of the anchor's \"\"\"\n",
    "    def coverage(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns the anchor coverage (a float)\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor coverage until a\n",
    "            certain index. For example, if the anchor has coverages\n",
    "            [0.1, 0.5, 0.95] and partial_index=1, this will return 0.5\n",
    "        \"\"\"\n",
    "        coverage = self.exp_map['coverage']\n",
    "        if len(coverage) == 0:\n",
    "            return 1\n",
    "        if partial_index is not None:\n",
    "            return coverage[partial_index]\n",
    "        else:\n",
    "            return coverage[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparative-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  anchor_base.py \n",
    "     https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py  \"\"\" \n",
    "from __future__ import print_function\n",
    "# import numpy as np\n",
    "import operator\n",
    "import copy\n",
    "# import sklearn\n",
    "# import collections\n",
    "\n",
    "\"\"\" Used in def get_anchor_from_tuple\"\"\"\n",
    "def matrix_subset(matrix, n_samples):\n",
    "    if matrix.shape[0] == 0:\n",
    "        return matrix\n",
    "    n_samples = min(matrix.shape[0], n_samples)\n",
    "    return matrix[np.random.choice(matrix.shape[0], n_samples, replace=False)]\n",
    "\n",
    "\"\"\" ~~~~ Used in calss AnchorTabularExplainer ~~~~ \"\"\"\n",
    "class AnchorBaseBeam(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Used in dlow_bernoulli \"\"\"\n",
    "    @staticmethod\n",
    "    def kl_bernoulli(p, q):\n",
    "        p = min(0.9999999999999999, max(0.0000001, p))\n",
    "        q = min(0.9999999999999999, max(0.0000001, q))\n",
    "        return (p * np.log(float(p) / q) + (1 - p) *\n",
    "                np.log(float(1 - p) / (1 - q)))\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def compute_beta(n_features, t, delta):\n",
    "        alpha = 1.1\n",
    "        k = 405.5\n",
    "        temp = np.log(k * n_features * (t ** alpha) / delta)\n",
    "        return temp + np.log(temp)\n",
    "    \n",
    "    \"\"\" Used in lucb and anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def dup_bernoulli(p, level):\n",
    "        lm = p\n",
    "        um = min(min(1, p + np.sqrt(level / 2.)), 1)\n",
    "        qm = (um + lm) / 2.\n",
    "#         print 'lm', lm, 'qm', qm, kl_bernoulli(p, qm)\n",
    "        if AnchorBaseBeam.kl_bernoulli(p, qm) > level:\n",
    "            um = qm\n",
    "        else:\n",
    "            lm = qm\n",
    "        return um\n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def lucb(sample_fns, initial_stats, epsilon, delta, batch_size, top_n,\n",
    "             verbose=False, verbose_every=1):\n",
    "        # initial_stats must have n_samples, positive\n",
    "        n_features = len(sample_fns)\n",
    "        n_samples = np.array(initial_stats['n_samples'])\n",
    "        positives = np.array(initial_stats['positives'])\n",
    "        ub = np.zeros(n_samples.shape)\n",
    "        lb = np.zeros(n_samples.shape)\n",
    "        for f in np.where(n_samples == 0)[0]:\n",
    "            n_samples[f] += 1\n",
    "            positives[f] += sample_fns[f](1)\n",
    "        if n_features == top_n:\n",
    "            return range(n_features)\n",
    "        means = positives / n_samples\n",
    "        t = 1\n",
    "\n",
    "        def update_bounds(t):\n",
    "            sorted_means = np.argsort(means)\n",
    "            beta = AnchorBaseBeam.compute_beta(n_features, t, delta)\n",
    "            J = sorted_means[-top_n:]\n",
    "            not_J = sorted_means[:-top_n]\n",
    "            for f in not_J:\n",
    "                ub[f] = AnchorBaseBeam.dup_bernoulli(means[f], beta /\n",
    "                                                     n_samples[f])\n",
    "            for f in J:\n",
    "                lb[f] = AnchorBaseBeam.dlow_bernoulli(means[f],\n",
    "                                                      beta / n_samples[f])\n",
    "            ut = not_J[np.argmax(ub[not_J])]\n",
    "            lt = J[np.argmin(lb[J])]\n",
    "            return ut, lt\n",
    "        ut, lt = update_bounds(t)\n",
    "        B = ub[ut] - lb[lt]\n",
    "        verbose_count = 0\n",
    "        while B > epsilon:\n",
    "            verbose_count += 1\n",
    "            if verbose and verbose_count % verbose_every == 0:\n",
    "                print('Best: %d (mean:%.10f, n: %d, lb:%.4f)' %\n",
    "                      (lt, means[lt], n_samples[lt], lb[lt]), end=' ')\n",
    "                print('Worst: %d (mean:%.4f, n: %d, ub:%.4f)' %\n",
    "                      (ut, means[ut], n_samples[ut], ub[ut]), end=' ')\n",
    "                print('B = %.2f' % B)\n",
    "            n_samples[ut] += batch_size\n",
    "            positives[ut] += sample_fns[ut](batch_size)\n",
    "            means[ut] = positives[ut] / n_samples[ut]\n",
    "            n_samples[lt] += batch_size\n",
    "            positives[lt] += sample_fns[lt](batch_size)\n",
    "            means[lt] = positives[lt] / n_samples[lt]\n",
    "            t += 1\n",
    "            ut, lt = update_bounds(t)\n",
    "            B = ub[ut] - lb[lt]\n",
    "        sorted_means = np.argsort(means)\n",
    "        return sorted_means[-top_n:]\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def dlow_bernoulli(p, level):\n",
    "        um = p\n",
    "        lm = max(min(1, p - np.sqrt(level / 2.)), 0)\n",
    "        qm = (um + lm) / 2.\n",
    "#         print 'lm', lm, 'qm', qm, kl_bernoulli(p, qm)\n",
    "        if AnchorBaseBeam.kl_bernoulli(p, qm) > level:\n",
    "            lm = qm\n",
    "        else:\n",
    "            um = qm\n",
    "        return lm\n",
    "\n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def make_tuples(previous_best, state):\n",
    "        # alters state, computes support for new tuples\n",
    "        normalize_tuple = lambda x: tuple(sorted(set(x)))  # noqa\n",
    "        all_features = range(state['n_features'])\n",
    "        coverage_data = state['coverage_data']\n",
    "        current_idx = state['current_idx']\n",
    "        data = state['data'][:current_idx]\n",
    "        labels = state['labels'][:current_idx]\n",
    "        if len(previous_best) == 0:\n",
    "            tuples = [(x, ) for x in all_features]\n",
    "            for x in tuples:\n",
    "                pres = data[:, x[0]].nonzero()[0]\n",
    "                # NEW\n",
    "                state['t_idx'][x] = set(pres)\n",
    "                state['t_nsamples'][x] = float(len(pres))\n",
    "                state['t_positives'][x] = float(labels[pres].sum())\n",
    "                state['t_order'][x].append(x[0])\n",
    "                # NEW\n",
    "                state['t_coverage_idx'][x] = set(\n",
    "                    coverage_data[:, x[0]].nonzero()[0])\n",
    "                state['t_coverage'][x] = (\n",
    "                    float(len(state['t_coverage_idx'][x])) /\n",
    "                    coverage_data.shape[0])\n",
    "            return tuples\n",
    "        new_tuples = set()\n",
    "        for f in all_features:\n",
    "            for t in previous_best:\n",
    "                new_t = normalize_tuple(t + (f, ))\n",
    "                if len(new_t) != len(t) + 1:\n",
    "                    continue\n",
    "                if new_t not in new_tuples:\n",
    "                    new_tuples.add(new_t)\n",
    "                    state['t_order'][new_t] = copy.deepcopy(state['t_order'][t])\n",
    "                    state['t_order'][new_t].append(f)\n",
    "                    state['t_coverage_idx'][new_t] = (\n",
    "                        state['t_coverage_idx'][t].intersection(\n",
    "                            state['t_coverage_idx'][(f,)]))\n",
    "                    state['t_coverage'][new_t] = (\n",
    "                        float(len(state['t_coverage_idx'][new_t])) /\n",
    "                        coverage_data.shape[0])\n",
    "                    t_idx = np.array(list(state['t_idx'][t]))\n",
    "                    t_data = state['data'][t_idx]\n",
    "                    present = np.where(t_data[:, f] == 1)[0]\n",
    "                    state['t_idx'][new_t] = set(t_idx[present])\n",
    "                    idx_list = list(state['t_idx'][new_t])\n",
    "                    state['t_nsamples'][new_t] = float(len(idx_list))\n",
    "                    state['t_positives'][new_t] = np.sum(\n",
    "                        state['labels'][idx_list])\n",
    "        return list(new_tuples)\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_sample_fns(sample_fn, tuples, state):\n",
    "        # each sample fn returns number of positives\n",
    "        sample_fns = []\n",
    "        def complete_sample_fn(t, n):\n",
    "            raw_data, data, labels = sample_fn(list(t), n)\n",
    "            current_idx = state['current_idx']\n",
    "            # idxs = range(state['data'].shape[0], state['data'].shape[0] + n)\n",
    "            idxs = range(current_idx, current_idx + n)\n",
    "            state['t_idx'][t].update(idxs)\n",
    "            state['t_nsamples'][t] += n\n",
    "            state['t_positives'][t] += labels.sum()\n",
    "            state['data'][idxs] = data\n",
    "            state['raw_data'][idxs] = raw_data\n",
    "            state['labels'][idxs] = labels\n",
    "            state['current_idx'] += n\n",
    "            if state['current_idx'] >= state['data'].shape[0] - max(1000, n):\n",
    "                prealloc_size = state['prealloc_size']\n",
    "                current_idx = data.shape[0]\n",
    "                state['data'] = np.vstack(\n",
    "                    (state['data'],\n",
    "                     np.zeros((prealloc_size, data.shape[1]), data.dtype)))\n",
    "                state['raw_data'] = np.vstack(\n",
    "                    (state['raw_data'],\n",
    "                     np.zeros((prealloc_size, raw_data.shape[1]),\n",
    "                              raw_data.dtype)))\n",
    "                state['labels'] = np.hstack(\n",
    "                    (state['labels'],\n",
    "                     np.zeros(prealloc_size, labels.dtype)))\n",
    "            # This can be really slow\n",
    "            # state['data'] = np.vstack((state['data'], data))\n",
    "            # state['raw_data'] = np.vstack((state['raw_data'], raw_data))\n",
    "            # state['labels'] = np.hstack((state['labels'], labels))\n",
    "            return labels.sum()\n",
    "        for t in tuples:\n",
    "            sample_fns.append(lambda n, t=t: complete_sample_fn(t, n))\n",
    "        return sample_fns\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_initial_statistics(tuples, state):\n",
    "        stats = {\n",
    "            'n_samples': [],\n",
    "            'positives': []\n",
    "        }\n",
    "        for t in tuples:\n",
    "            stats['n_samples'].append(state['t_nsamples'][t])\n",
    "            stats['positives'].append(state['t_positives'][t])\n",
    "        return stats\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_anchor_from_tuple(t, state):\n",
    "        # TODO: This is wrong, some of the intermediate anchors may not exist.\n",
    "        anchor = {'feature': [], 'mean': [], 'precision': [],\n",
    "                  'coverage': [], 'examples': [], 'all_precision': 0}\n",
    "        anchor['num_preds'] = state['data'].shape[0]\n",
    "        normalize_tuple = lambda x: tuple(sorted(set(x)))  # noqa\n",
    "        current_t = tuple()\n",
    "        for f in state['t_order'][t]:\n",
    "            current_t = normalize_tuple(current_t + (f,))\n",
    "\n",
    "            mean = (state['t_positives'][current_t] /\n",
    "                    state['t_nsamples'][current_t])\n",
    "            anchor['feature'].append(f)\n",
    "            anchor['mean'].append(mean)\n",
    "            anchor['precision'].append(mean)\n",
    "            anchor['coverage'].append(state['t_coverage'][current_t])\n",
    "            raw_idx = list(state['t_idx'][current_t])\n",
    "            raw_data = state['raw_data'][raw_idx]\n",
    "            covered_true = (\n",
    "                state['raw_data'][raw_idx][state['labels'][raw_idx] == 1])\n",
    "            covered_false = (\n",
    "                state['raw_data'][raw_idx][state['labels'][raw_idx] == 0])\n",
    "            exs = {}\n",
    "            exs['covered'] = matrix_subset(raw_data, 10)\n",
    "            exs['covered_true'] = matrix_subset(covered_true, 10)\n",
    "            exs['covered_false'] = matrix_subset(covered_false, 10)\n",
    "            exs['uncovered_true'] = np.array([])\n",
    "            exs['uncovered_false'] = np.array([])\n",
    "            anchor['examples'].append(exs)\n",
    "        return anchor\n",
    "    \n",
    "    \"\"\" I think is where the best anchor is selected (using mult)\"\"\"\n",
    "    @staticmethod\n",
    "    def anchor_beam(sample_fn, delta=0.05, epsilon=0.1, batch_size=10,\n",
    "                    min_shared_samples=0, desired_confidence=1, beam_size=1,\n",
    "                    verbose=False, epsilon_stop=0.05, min_samples_start=0,\n",
    "                    max_anchor_size=None, verbose_every=1,\n",
    "                    stop_on_first=False, coverage_samples=10000):\n",
    "        anchor = {'feature': [], 'mean': [], 'precision': [],\n",
    "                  'coverage': [], 'examples': [], 'all_precision': 0}\n",
    "        _, coverage_data, _ = sample_fn([], coverage_samples, compute_labels=False)\n",
    "        raw_data, data, labels = sample_fn([], max(1, min_samples_start))\n",
    "        mean = labels.mean()\n",
    "        beta = np.log(1. / delta)\n",
    "        lb = AnchorBaseBeam.dlow_bernoulli(mean, beta / data.shape[0])\n",
    "        while mean > desired_confidence and lb < desired_confidence - epsilon:\n",
    "            nraw_data, ndata, nlabels = sample_fn([], batch_size)\n",
    "            data = np.vstack((data, ndata))\n",
    "            raw_data = np.vstack((raw_data, nraw_data))\n",
    "            labels = np.hstack((labels, nlabels))\n",
    "            mean = labels.mean()\n",
    "            lb = AnchorBaseBeam.dlow_bernoulli(mean, beta / data.shape[0])\n",
    "        if lb > desired_confidence:\n",
    "            anchor['num_preds'] = data.shape[0]\n",
    "            anchor['all_precision'] = mean\n",
    "            return anchor\n",
    "        prealloc_size = batch_size * 10000\n",
    "        current_idx = data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((prealloc_size, data.shape[1]),\n",
    "                                         data.dtype)))\n",
    "        raw_data = np.vstack(\n",
    "            (raw_data, np.zeros((prealloc_size, raw_data.shape[1]),\n",
    "                                raw_data.dtype)))\n",
    "        labels = np.hstack((labels, np.zeros(prealloc_size, labels.dtype)))\n",
    "        n_features = data.shape[1]\n",
    "        state = {'t_idx': collections.defaultdict(lambda: set()),\n",
    "                 't_nsamples': collections.defaultdict(lambda: 0.),\n",
    "                 't_positives': collections.defaultdict(lambda: 0.),\n",
    "                 'data': data,\n",
    "                 'prealloc_size': prealloc_size,\n",
    "                 'raw_data': raw_data,\n",
    "                 'labels': labels,\n",
    "                 'current_idx': current_idx,\n",
    "                 'n_features': n_features,\n",
    "                 't_coverage_idx': collections.defaultdict(lambda: set()),\n",
    "                 't_coverage': collections.defaultdict(lambda: 0.),\n",
    "                 'coverage_data': coverage_data,\n",
    "                 't_order': collections.defaultdict(lambda: list())\n",
    "                 }\n",
    "        current_size = 1\n",
    "        best_of_size = {0: []}\n",
    "        best_coverage = -1\n",
    "        best_tuple = ()\n",
    "        t = 1\n",
    "        if max_anchor_size is None:\n",
    "            max_anchor_size = n_features\n",
    "        while current_size <= max_anchor_size:\n",
    "            tuples = AnchorBaseBeam.make_tuples(\n",
    "                best_of_size[current_size - 1], state)\n",
    "            tuples = [x for x in tuples\n",
    "                      if state['t_coverage'][x] > best_coverage]\n",
    "            if len(tuples) == 0:\n",
    "                break\n",
    "            sample_fns = AnchorBaseBeam.get_sample_fns(sample_fn, tuples,\n",
    "                                                       state)\n",
    "            initial_stats = AnchorBaseBeam.get_initial_statistics(tuples,\n",
    "                                                                  state)\n",
    "            # print tuples, beam_size\n",
    "            chosen_tuples = AnchorBaseBeam.lucb(\n",
    "                sample_fns, initial_stats, epsilon, delta, batch_size,\n",
    "                min(beam_size, len(tuples)),\n",
    "                verbose=verbose, verbose_every=verbose_every)\n",
    "            best_of_size[current_size] = [tuples[x] for x in chosen_tuples]\n",
    "            if verbose:\n",
    "                print('Best of size ', current_size, ':')\n",
    "            # print state['data'].shape[0]\n",
    "            stop_this = False\n",
    "            for i, t in zip(chosen_tuples, best_of_size[current_size]):\n",
    "                # I can choose at most (beam_size - 1) tuples at each step,\n",
    "                # and there are at most n_feature steps\n",
    "                beta = np.log(1. /\n",
    "                              (delta / (1 + (beam_size - 1) * n_features)))\n",
    "                # beta = np.log(1. / delta)\n",
    "                # if state['t_nsamples'][t] == 0:\n",
    "                #     mean = 1\n",
    "                # else:\n",
    "                mean = state['t_positives'][t] / state['t_nsamples'][t]\n",
    "                lb = AnchorBaseBeam.dlow_bernoulli(\n",
    "                    mean, beta / state['t_nsamples'][t])\n",
    "                ub = AnchorBaseBeam.dup_bernoulli(\n",
    "                    mean, beta / state['t_nsamples'][t])\n",
    "                coverage = state['t_coverage'][t]\n",
    "                if verbose:\n",
    "                    print(i, mean, lb, ub)\n",
    "                while ((mean >= desired_confidence and\n",
    "                       lb < desired_confidence - epsilon_stop) or\n",
    "                       (mean < desired_confidence and\n",
    "                        ub >= desired_confidence + epsilon_stop)):\n",
    "                    # print mean, lb, state['t_nsamples'][t]\n",
    "                    sample_fns[i](batch_size)\n",
    "                    mean = state['t_positives'][t] / state['t_nsamples'][t]\n",
    "                    lb = AnchorBaseBeam.dlow_bernoulli(\n",
    "                        mean, beta / state['t_nsamples'][t])\n",
    "                    ub = AnchorBaseBeam.dup_bernoulli(\n",
    "                        mean, beta / state['t_nsamples'][t])\n",
    "                if verbose:\n",
    "                    print('%s mean = %.2f lb = %.2f ub = %.2f coverage: %.2f n: %d' % (t, mean, lb, ub, coverage, state['t_nsamples'][t]))\n",
    "                if mean >= desired_confidence and lb > desired_confidence - epsilon_stop:\n",
    "                    if verbose:\n",
    "                        print('Found eligible anchor ', t, 'Coverage:',\n",
    "                              coverage, 'Is best?', coverage > best_coverage)\n",
    "                    if coverage > best_coverage:\n",
    "                        best_coverage = coverage\n",
    "                        best_tuple = t\n",
    "                        if best_coverage == 1 or stop_on_first:\n",
    "                            stop_this = True\n",
    "            if stop_this:\n",
    "                break\n",
    "            current_size += 1\n",
    "        if best_tuple == ():\n",
    "            # Could not find an anchor, will now choose the highest precision\n",
    "            # amongst the top K from every round\n",
    "            if verbose:\n",
    "                print('Could not find an anchor, now doing best of each size')\n",
    "            tuples = []\n",
    "            for i in range(0, current_size):\n",
    "                tuples.extend(best_of_size[i])\n",
    "            # tuples = best_of_size[current_size - 1]\n",
    "            sample_fns = AnchorBaseBeam.get_sample_fns(sample_fn, tuples,\n",
    "                                                       state)\n",
    "            initial_stats = AnchorBaseBeam.get_initial_statistics(tuples,\n",
    "                                                                  state)\n",
    "            # print tuples, beam_size\n",
    "            chosen_tuples = AnchorBaseBeam.lucb(\n",
    "                sample_fns, initial_stats, epsilon, delta, batch_size,\n",
    "                1, verbose=verbose)\n",
    "            best_tuple = tuples[chosen_tuples[0]]\n",
    "        # return best_tuple, state\n",
    "        return AnchorBaseBeam.get_anchor_from_tuple(best_tuple, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "great-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" anchor_tabular.py \n",
    "    https://github.com/marcotcr/anchor/blob/master/anchor/anchor_tabular.py\"\"\"\n",
    "\n",
    "\"\"\"def as_html(, def jsonize(x)\"\"\"\n",
    "\n",
    "def id_generator(size=15):\n",
    "    \"\"\"Helper function to generate random div ids. This is useful for embedding\n",
    "    HTML into ipython notebooks.\"\"\"\n",
    "    chars = list(string.ascii_uppercase + string.digits)\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" This is the main class-file to compute the anchors for tabular data \"\"\"\n",
    "class AnchorTabularExplainer(object):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            class_names: list of strings\n",
    "            feature_names: list of strings\n",
    "            train_data: used to sample (bootstrap)\n",
    "            categorical_names: map from integer to list of strings, names for each\n",
    "                value of the categorical features. Every feature that is not in\n",
    "                this map will be considered as ordinal or continuous, and thus discretized.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, class_names, feature_names, train_data,\n",
    "                 categorical_names={}, discretizer='quartile', encoder_fn=None):\n",
    "        self.min = {}\n",
    "        self.max = {}\n",
    "        self.disc = collections.namedtuple('random_name2',\n",
    "                                              ['discretize'])(lambda x: x)\n",
    "        self.encoder_fn = lambda x: x\n",
    "        if encoder_fn is not None:\n",
    "            self.encoder_fn = encoder_fn\n",
    "        self.categorical_features = []\n",
    "        self.feature_names = feature_names\n",
    "        self.train = train_data\n",
    "        self.class_names = class_names\n",
    "        self.categorical_names = copy.deepcopy(categorical_names)\n",
    "        if categorical_names:\n",
    "            self.categorical_features = sorted(categorical_names.keys())\n",
    "\n",
    "        if discretizer == 'quartile':\n",
    "            self.disc = lime.lime_tabular.QuartileDiscretizer(train_data,\n",
    "                                                         self.categorical_features,\n",
    "                                                         self.feature_names)\n",
    "        elif discretizer == 'decile':\n",
    "            self.disc = lime.lime_tabular.DecileDiscretizer(train_data,\n",
    "                                                     self.categorical_features,\n",
    "                                                     self.feature_names)\n",
    "        else:\n",
    "            raise ValueError('Discretizer must be quartile or decile')\n",
    "\n",
    "        self.ordinal_features = [x for x in range(len(feature_names)) if x not in self.categorical_features]\n",
    "\n",
    "        self.d_train = self.disc.discretize(self.train)\n",
    "        self.categorical_names.update(self.disc.names)\n",
    "        self.categorical_features += self.ordinal_features\n",
    "\n",
    "        for f in range(train_data.shape[1]):\n",
    "            self.min[f] = np.min(train_data[:, f])\n",
    "            self.max[f] = np.max(train_data[:, f])\n",
    "            \n",
    "    \n",
    "    \"\"\" This is def explain_instance: def sample_fn \"\"\"\n",
    "    def sample_from_train(self, conditions_eq, conditions_neq, conditions_geq,\n",
    "                          conditions_leq, num_samples):\n",
    "        \"\"\"\n",
    "        bla\n",
    "        \"\"\"\n",
    "        train = self.train\n",
    "        d_train = self.d_train\n",
    "        idx = np.random.choice(range(train.shape[0]), num_samples,\n",
    "                               replace=True)\n",
    "        sample = train[idx]\n",
    "        d_sample = d_train[idx]\n",
    "        for f in conditions_eq:\n",
    "            sample[:, f] = np.repeat(conditions_eq[f], num_samples)\n",
    "        for f in conditions_geq:\n",
    "            idx = d_sample[:, f] <= conditions_geq[f]\n",
    "            if f in conditions_leq:\n",
    "                idx = (idx + (d_sample[:, f] > conditions_leq[f])).astype(bool)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            options = d_train[:, f] > conditions_geq[f]\n",
    "            if f in conditions_leq:\n",
    "                options = options * (d_train[:, f] <= conditions_leq[f])\n",
    "            if options.sum() == 0:\n",
    "                min_ = conditions_geq.get(f, self.min[f])\n",
    "                max_ = conditions_leq.get(f, self.max[f])\n",
    "                to_rep = np.random.uniform(min_, max_, idx.sum())\n",
    "            else:\n",
    "                to_rep = np.random.choice(train[options, f], idx.sum(),\n",
    "                                          replace=True)\n",
    "            sample[idx, f] = to_rep\n",
    "        for f in conditions_leq:\n",
    "            if f in conditions_geq:\n",
    "                continue\n",
    "            idx = d_sample[:, f] > conditions_leq[f]\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            options = d_train[:, f] <= conditions_leq[f]\n",
    "            if options.sum() == 0:\n",
    "                min_ = conditions_geq.get(f, self.min[f])\n",
    "                max_ = conditions_leq.get(f, self.max[f])\n",
    "                to_rep = np.random.uniform(min_, max_, idx.sum())\n",
    "            else:\n",
    "                to_rep = np.random.choice(train[options, f], idx.sum(),\n",
    "                                          replace=True)\n",
    "            sample[idx, f] = to_rep\n",
    "        return sample\n",
    "\n",
    "    \"\"\" Used in get_sample_fn: def sample_fn \"\"\"\n",
    "    def as_html(self, exp, **kwargs):\n",
    "        \"\"\"bla\"\"\"\n",
    "        exp_map = self.to_explanation_map(exp)\n",
    "\n",
    "        def jsonize(x): return json.dumps(x)\n",
    "        this_dir, _ = os.path.split(__file__)\n",
    "        bundle = open(os.path.join(this_dir, 'bundle.js'), encoding='utf8').read()\n",
    "        random_id = 'top_div' + id_generator()\n",
    "        out = u'''<html>\n",
    "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n",
    "        <head><script>%s </script></head><body>''' % bundle\n",
    "        out += u'''\n",
    "        <div id=\"{random_id}\" />\n",
    "        <script>\n",
    "            div = d3.select(\"#{random_id}\");\n",
    "            lime.RenderExplanationFrame(div,{label_names}, {predict_proba},\n",
    "            {true_class}, {explanation}, {raw_data}, \"tabular\", {explanation_type});\n",
    "        </script>'''.format(random_id=random_id,\n",
    "                            label_names=jsonize(exp_map['labelNames']),\n",
    "                            predict_proba=jsonize(exp_map['predictProba']),\n",
    "                            true_class=jsonize(exp_map['trueClass']),\n",
    "                            explanation=jsonize(exp_map['explanation']),\n",
    "                            raw_data=jsonize(exp_map['rawData']),\n",
    "                            explanation_type=jsonize(exp_map['explanationType']))\n",
    "        out += u'</body></html>'\n",
    "        return out\n",
    "    \n",
    "    \"\"\" Used in def explain_instance \"\"\"\n",
    "    def get_sample_fn(self, data_row, classifier_fn, desired_label=None):\n",
    "        def predict_fn(x):\n",
    "            return classifier_fn(self.encoder_fn(x))\n",
    "        true_label = desired_label\n",
    "        if true_label is None:\n",
    "            true_label = predict_fn(data_row.reshape(1, -1))[0]\n",
    "        # must map present here to include categorical features (for conditions_eq), and numerical features for geq and leq\n",
    "        mapping = {}\n",
    "        data_row = self.disc.discretize(data_row.reshape(1, -1))[0]\n",
    "        for f in self.categorical_features:\n",
    "            if f in self.ordinal_features:\n",
    "                for v in range(len(self.categorical_names[f])):\n",
    "                    idx = len(mapping)\n",
    "                    if data_row[f] <= v and v != len(self.categorical_names[f]) - 1:\n",
    "                        mapping[idx] = (f, 'leq', v)\n",
    "                        # names[idx] = '%s <= %s' % (self.feature_names[f], v)\n",
    "                    elif data_row[f] > v:\n",
    "                        mapping[idx] = (f, 'geq', v)\n",
    "                        # names[idx] = '%s > %s' % (self.feature_names[f], v)\n",
    "            else:\n",
    "                idx = len(mapping)\n",
    "                mapping[idx] = (f, 'eq', data_row[f])\n",
    "            # names[idx] = '%s = %s' % (\n",
    "            #     self.feature_names[f],\n",
    "            #     self.categorical_names[f][int(data_row[f])])\n",
    "\n",
    "        def sample_fn(present, num_samples, compute_labels=True):\n",
    "            conditions_eq = {}\n",
    "            conditions_leq = {}\n",
    "            conditions_geq = {}\n",
    "            for x in present:\n",
    "                f, op, v = mapping[x]\n",
    "                if op == 'eq':\n",
    "                    conditions_eq[f] = v\n",
    "                if op == 'leq':\n",
    "                    if f not in conditions_leq:\n",
    "                        conditions_leq[f] = v\n",
    "                    conditions_leq[f] = min(conditions_leq[f], v)\n",
    "                if op == 'geq':\n",
    "                    if f not in conditions_geq:\n",
    "                        conditions_geq[f] = v\n",
    "                    conditions_geq[f] = max(conditions_geq[f], v)\n",
    "            # conditions_eq = dict([(x, data_row[x]) for x in present])\n",
    "            raw_data = self.sample_from_train(\n",
    "                conditions_eq, {}, conditions_geq, conditions_leq, num_samples)\n",
    "            d_raw_data = self.disc.discretize(raw_data)\n",
    "            data = np.zeros((num_samples, len(mapping)), int)\n",
    "            for i in mapping:\n",
    "                f, op, v = mapping[i]\n",
    "                if op == 'eq':\n",
    "                    data[:, i] = (d_raw_data[:, f] == data_row[f]).astype(int)\n",
    "                if op == 'leq':\n",
    "                    data[:, i] = (d_raw_data[:, f] <= v).astype(int)\n",
    "                if op == 'geq':\n",
    "                    data[:, i] = (d_raw_data[:, f] > v).astype(int)\n",
    "            # data = (raw_data == data_row).astype(int)\n",
    "            labels = []\n",
    "            if compute_labels:\n",
    "                labels = (predict_fn(raw_data) == true_label).astype(int)\n",
    "            return raw_data, data, labels\n",
    "        return sample_fn, mapping\n",
    "        \n",
    "    \"\"\" Used outside to explain an instance, this is the core functin, where all of the explanabily can happends\"\"\"\n",
    "    def explain_instance(self, data_row, classifier_fn, threshold=0.95,\n",
    "                          delta=0.1, tau=0.15, batch_size=100,\n",
    "                          max_anchor_size=None,\n",
    "                          desired_label=None,\n",
    "                          beam_size=4, **kwargs):\n",
    "        # It's possible to pass in max_anchor_size\n",
    "        sample_fn, mapping = self.get_sample_fn(\n",
    "            data_row, classifier_fn, desired_label=desired_label)\n",
    "        # return sample_fn, mapping\n",
    "        \"\"\" Changing exp = anchor_base.AnchorBaseBeam.anchor_beam(...) \n",
    "                to   exp = AnchorBaseBeam.anchor_beam(...) \n",
    "                since class is not imported any more \"\"\"\n",
    "        exp = AnchorBaseBeam.anchor_beam(\n",
    "            sample_fn, delta=delta, epsilon=tau, batch_size=batch_size,\n",
    "            desired_confidence=threshold, max_anchor_size=max_anchor_size,\n",
    "            **kwargs)\n",
    "        self.add_names_to_exp(data_row, exp, mapping)\n",
    "        exp['instance'] = data_row\n",
    "        exp['prediction'] = classifier_fn(self.encoder_fn(data_row.reshape(1, -1)))[0]\n",
    "        \n",
    "        \"\"\" cahnge: explanation = anchor_explanation.AnchorExplanation('tabular', exp, self.as_html)\n",
    "                to: explanation = AnchorExplanation('tabular', exp, self.as_html) \n",
    "                Not imported but class defined \"\"\"\n",
    "        explanation = AnchorExplanation('tabular', exp, self.as_html)\n",
    "        return explanation\n",
    "    \n",
    "    \"\"\" Used in explain_instance \"\"\"\n",
    "    def add_names_to_exp(self, data_row, hoeffding_exp, mapping):\n",
    "        # TODO: precision recall is all wrong, coverage functions wont work\n",
    "        # anymore due to ranges\n",
    "        idxs = hoeffding_exp['feature']\n",
    "        hoeffding_exp['names'] = []\n",
    "        hoeffding_exp['feature'] = [mapping[idx][0] for idx in idxs]\n",
    "        ordinal_ranges = {}\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            if op == 'geq' or op == 'leq':\n",
    "                if f not in ordinal_ranges:\n",
    "                    ordinal_ranges[f] = [float('-inf'), float('inf')]\n",
    "            if op == 'geq':\n",
    "                ordinal_ranges[f][0] = max(ordinal_ranges[f][0], v)\n",
    "            if op == 'leq':\n",
    "                ordinal_ranges[f][1] = min(ordinal_ranges[f][1], v)\n",
    "        handled = set()\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            # v = data_row[f]\n",
    "            if op == 'eq':\n",
    "                fname = '%s = ' % self.feature_names[f]\n",
    "                if f in self.categorical_names:\n",
    "                    v = int(v)\n",
    "                    if ('<' in self.categorical_names[f][v]\n",
    "                            or '>' in self.categorical_names[f][v]):\n",
    "                        fname = ''\n",
    "                    fname = '%s%s' % (fname, self.categorical_names[f][v])\n",
    "                else:\n",
    "                    fname = '%s%.2f' % (fname, v)\n",
    "            else:\n",
    "                if f in handled:\n",
    "                    continue\n",
    "                geq, leq = ordinal_ranges[f]\n",
    "                fname = ''\n",
    "                geq_val = ''\n",
    "                leq_val = ''\n",
    "                if geq > float('-inf'):\n",
    "                    if geq == len(self.categorical_names[f]) - 1:\n",
    "                        geq = geq - 1\n",
    "                    name = self.categorical_names[f][geq + 1]\n",
    "                    if '<' in name:\n",
    "                        geq_val = name.split()[0]\n",
    "                    elif '>' in name:\n",
    "                        geq_val = name.split()[-1]\n",
    "                if leq < float('inf'):\n",
    "                    name = self.categorical_names[f][leq]\n",
    "                    if leq == 0:\n",
    "                        leq_val = name.split()[-1]\n",
    "                    elif '<' in name:\n",
    "                        leq_val = name.split()[-1]\n",
    "                if leq_val and geq_val:\n",
    "                    fname = '%s < %s <= %s' % (geq_val, self.feature_names[f],\n",
    "                                               leq_val)\n",
    "                elif leq_val:\n",
    "                    fname = '%s <= %s' % (self.feature_names[f], leq_val)\n",
    "                elif geq_val:\n",
    "                    fname = '%s > %s' % (self.feature_names[f], geq_val)\n",
    "                handled.add(f)\n",
    "            hoeffding_exp['names'].append(fname)\n",
    "   \n",
    "    \"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  NOT NEED IT  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
    "    \"\"\" This functions in not used at all in the code \"\"\"\n",
    "    def transform_to_examples(self, examples, features_in_anchor=[],\n",
    "                              predicted_label=None):\n",
    "        ret_obj = []\n",
    "        if len(examples) == 0:\n",
    "            return ret_obj\n",
    "        weights = [int(predicted_label) if x in features_in_anchor else -1\n",
    "                   for x in range(examples.shape[1])]\n",
    "        examples = self.disc.discretize(examples)\n",
    "        for ex in examples:\n",
    "            values = [self.categorical_names[i][int(ex[i])]\n",
    "                      if i in self.categorical_features\n",
    "                      else ex[i] for i in range(ex.shape[0])]\n",
    "            ret_obj.append(list(zip(self.feature_names, values, weights)))\n",
    "        return ret_obj\n",
    "    \n",
    "    def add_names_to_exp(self, data_row, hoeffding_exp, mapping):\n",
    "        # TODO: precision recall is all wrong, coverage functions wont work\n",
    "        # anymore due to ranges\n",
    "        idxs = hoeffding_exp['feature']\n",
    "        hoeffding_exp['names'] = []\n",
    "        hoeffding_exp['feature'] = [mapping[idx][0] for idx in idxs]\n",
    "        ordinal_ranges = {}\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            if op == 'geq' or op == 'leq':\n",
    "                if f not in ordinal_ranges:\n",
    "                    ordinal_ranges[f] = [float('-inf'), float('inf')]\n",
    "            if op == 'geq':\n",
    "                ordinal_ranges[f][0] = max(ordinal_ranges[f][0], v)\n",
    "            if op == 'leq':\n",
    "                ordinal_ranges[f][1] = min(ordinal_ranges[f][1], v)\n",
    "        handled = set()\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            # v = data_row[f]\n",
    "            if op == 'eq':\n",
    "                fname = '%s = ' % self.feature_names[f]\n",
    "                if f in self.categorical_names:\n",
    "                    v = int(v)\n",
    "                    if ('<' in self.categorical_names[f][v]\n",
    "                            or '>' in self.categorical_names[f][v]):\n",
    "                        fname = ''\n",
    "                    fname = '%s%s' % (fname, self.categorical_names[f][v])\n",
    "                else:\n",
    "                    fname = '%s%.2f' % (fname, v)\n",
    "            else:\n",
    "                if f in handled:\n",
    "                    continue\n",
    "                geq, leq = ordinal_ranges[f]\n",
    "                fname = ''\n",
    "                geq_val = ''\n",
    "                leq_val = ''\n",
    "                if geq > float('-inf'):\n",
    "                    if geq == len(self.categorical_names[f]) - 1:\n",
    "                        geq = geq - 1\n",
    "                    name = self.categorical_names[f][geq + 1]\n",
    "                    if '<' in name:\n",
    "                        geq_val = name.split()[0]\n",
    "                    elif '>' in name:\n",
    "                        geq_val = name.split()[-1]\n",
    "                if leq < float('inf'):\n",
    "                    name = self.categorical_names[f][leq]\n",
    "                    if leq == 0:\n",
    "                        leq_val = name.split()[-1]\n",
    "                    elif '<' in name:\n",
    "                        leq_val = name.split()[-1]\n",
    "                if leq_val and geq_val:\n",
    "                    fname = '%s < %s <= %s' % (geq_val, self.feature_names[f],\n",
    "                                               leq_val)\n",
    "                elif leq_val:\n",
    "                    fname = '%s <= %s' % (self.feature_names[f], leq_val)\n",
    "                elif geq_val:\n",
    "                    fname = '%s > %s' % (self.feature_names[f], geq_val)\n",
    "                handled.add(f)\n",
    "            hoeffding_exp['names'].append(fname)\n",
    "    \"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ END - NOT NEED IT  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
    "\n",
    "# \"\"\" We do not use  def transform_to_examples and  def to_explanation_map(self, exp)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-implement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-angel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secret-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if these two function is sufficient to implement a simple ANCHOR in the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prepared-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fatf\n",
    "import fatf.utils.data.datasets as fatf_datasets\n",
    "\n",
    "iris_data_dict = fatf_datasets.load_iris()\n",
    "iris_data = iris_data_dict['data']\n",
    "iris_target = iris_data_dict['target']\n",
    "iris_feature_names = iris_data_dict['feature_names'].tolist()\n",
    "iris_target_names = iris_data_dict['target_names'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "national-cigarette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1.0\n",
      "Test 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(iris_data, iris_target, train_size=0.80)\n",
    "blackbox_model = sklearn.ensemble.RandomForestClassifier(n_estimators=20)\n",
    "blackbox_model.fit(train, labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(labels_train, blackbox_model.predict(train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(labels_test, blackbox_model.predict(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continental-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorTabularExplainer(\n",
    "    iris_target_names,\n",
    "    iris_feature_names,\n",
    "    train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lonely-people",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  setosa\n"
     ]
    }
   ],
   "source": [
    "idx = 13\n",
    "np.random.seed(1)\n",
    "print('Prediction: ', explainer.class_names[int(blackbox_model.predict(test[idx].reshape(1, -1))[0])])\n",
    "\"\"\" cahnge exp = explainer.explain_instance(...) \n",
    "    to AnchorExplanation \"\"\"\n",
    "exp = explainer.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "local-cotton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: petal length (cm) <= 1.50 AND sepal width (cm) > 3.40\n",
      "Precision: 1.00\n",
      "Coverage: 0.13\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrapped-flood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "precious-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor test precision: 1.00\n",
      "Anchor test coverage: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Get test examples where the anchora pplies\n",
    "fit_anchor = np.where(np.all(test[:, exp.features()] == test[idx][exp.features()], axis=1))[0]\n",
    "print('Anchor test precision: %.2f' % (np.mean(blackbox_model.predict(test[fit_anchor]) == blackbox_model.predict(test[idx].reshape(1, -1)))))\n",
    "print('Anchor test coverage: %.2f' % (fit_anchor.shape[0] / float(test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technical-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial anchor: petal length (cm) <= 1.50 AND sepal width (cm) > 3.40\n",
      "Partial precision: 1.00\n",
      "Partial coverage: 0.13\n"
     ]
    }
   ],
   "source": [
    "print('Partial anchor: %s' % (' AND '.join(exp.names(1))))\n",
    "print('Partial precision: %.2f' % exp.precision(1))\n",
    "print('Partial coverage: %.2f' % exp.coverage(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacterial-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial anchor test precision: 1.00\n",
      "Partial anchor test coverage: 0.03\n"
     ]
    }
   ],
   "source": [
    "fit_partial = np.where(np.all(test[:, exp.features(1)] == test[idx][exp.features(1)], axis=1))[0]\n",
    "print('Partial anchor test precision: %.2f' % (np.mean(blackbox_model.predict(test[fit_partial]) == blackbox_model.predict(test[idx].reshape(1, -1)))))\n",
    "print('Partial anchor test coverage: %.2f' % (fit_partial.shape[0] / float(test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-comment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-partition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-ballet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
