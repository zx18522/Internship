{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eastern-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fatf\n",
    "import fatf.utils.data.datasets as fatf_datasets\n",
    "from scipy import stats \n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "# %matplotlib notebook\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sitting-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "individual-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gorgeous-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.discretize import BaseDiscretizer\n",
    "class GaussianMixtureModelsDiscretizer(BaseDiscretizer):\n",
    "    \n",
    "    def __init__(self, train, categorical_features, feature_names, labels, random_state=None, data_stats=None):\n",
    "        self.train = train\n",
    "        self.labels = labels\n",
    "        \n",
    "        if(labels is None):\n",
    "            raise ValueError('Labels must be not None when using \\\n",
    "                             Gaussian Mixture Models Discretizer')\n",
    "            \n",
    "        BaseDiscretizer.__init__(self, train, categorical_features,\n",
    "                                 feature_names, labels=labels,\n",
    "                                 random_state=random_state,\n",
    "                                 data_stats=data_stats)\n",
    "\n",
    "    \n",
    "    def getLabels(self):\n",
    "        train_label = []\n",
    "        for lebel_number in set(self.labels):\n",
    "            Classification = np.array([self.train[i] for i in range(len(self.labels)) if self.labels[i] == lebel_number])\n",
    "            train_label.append(Classification)\n",
    "        \n",
    "        return train_label\n",
    "    \n",
    "    # BIC_range = make it an input??\n",
    "    def gmmBIC(self, train_label_list, BIC_range = [1, 10]):\n",
    "    \n",
    "        all_gmm_list = []\n",
    "        for class_C in train_label_list:\n",
    "            gmm_list = []\n",
    "            number_features = len(class_C[0])\n",
    "            for i in range(number_features): \n",
    "                BIC_list = []\n",
    "                for x in range(min(BIC_range), max(BIC_range) + 1):\n",
    "                    gmm = mixture.GaussianMixture(n_components= x, covariance_type='full').fit(class_C[:, i].reshape(-1, 1))\n",
    "                    BIC_list.append(gmm.bic(class_C[:, i].reshape(-1, 1)))\n",
    "\n",
    "                min_BIC_index = BIC_list.index(min(BIC_list))\n",
    "                number_components = min_BIC_index + 1\n",
    "                gmm = mixture.GaussianMixture(n_components=number_components, covariance_type='full').fit(class_C[:, i].reshape(-1, 1))\n",
    "                gmm_list.append(gmm)\n",
    "\n",
    "            all_gmm_list.append(gmm_list)\n",
    "\n",
    "        return all_gmm_list\n",
    "    \n",
    "    def getIntervalsGMM(self, gmm, std_constant=3):\n",
    "    \n",
    "        width_interval_list = []\n",
    "        pos_list = []\n",
    "        weight_list = []\n",
    "        for pos, covariance, weight in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "\n",
    "            width = [float(pos -std_constant * np.sqrt(covariance[0][0])), float(pos + std_constant * np.sqrt(covariance[0][0]))]\n",
    "\n",
    "            width_interval_list.append(width)\n",
    "            pos_list.append(pos)\n",
    "            weight_list.append(weight)\n",
    "\n",
    "\n",
    "        return pos_list, width_interval_list, weight_list\n",
    "    \n",
    "    \n",
    "    def discretizeGMM(self, gmm_all, std_constant=3):\n",
    "        \n",
    "        n_classes = len(gmm_all)\n",
    "        n_features = len(gmm_all[0])\n",
    "\n",
    "        features_discretized = []\n",
    "        for n in range(n_features):\n",
    "            inteval_features = []\n",
    "            for i in range(n_classes):\n",
    "                _, intevals_list, _  = self.getIntervalsGMM(gmm_all[i][n], std_constant)\n",
    "                flat_intervals = sorted([interval for intervals in intevals_list for interval in intervals])\n",
    "                inteval_features.append(flat_intervals)\n",
    "\n",
    "            inteval_features_flat = sorted([interval for intervals in inteval_features for interval in intervals])\n",
    "            features_discretized.append(np.array(inteval_features_flat))\n",
    "\n",
    "\n",
    "        return features_discretized\n",
    "\n",
    "    \n",
    "      \n",
    "        \n",
    "    def bins(self, data, labels):\n",
    "        \n",
    "        train_label = self.getLabels()\n",
    "        all_gmm_list = self.gmmBIC(train_label)\n",
    "        feature_intervals_lists = self.discretizeGMM(all_gmm_list)\n",
    "        bins = feature_intervals_lists\n",
    "        \n",
    "        return bins\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "respected-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_path = r'Adult_data_set\\adult_data.txt'\n",
    "df = pd.read_csv(adult_path, header=None)\n",
    "\n",
    "adult_path_test = r'Adult_data_set\\adult_test.txt'\n",
    "df_test = pd.read_csv(adult_path_test, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "criminal-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = pd.get_dummies(df.iloc[:, -1])\n",
    "y = y_d[' >50K']\n",
    "\n",
    "y_d_test = pd.get_dummies(df_test.iloc[:, -1])\n",
    "y_test = y_d_test[' >50K.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "lovely-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Numerice(df_column):\n",
    "    unique = df_column.unique()\n",
    "    status_unique = dict(zip(unique, range(len(unique))))\n",
    "    df_column = df_column.replace(status_unique)\n",
    "    return df_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "angry-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = len(df.columns)\n",
    "\n",
    "for i in range(n_columns):\n",
    "    df[i]= Numerice(df[i])\n",
    "    df_test[i]= Numerice(df_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "hungarian-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :8]\n",
    "X_test = df_test.iloc[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "particular-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = list(y_d.columns)\n",
    "feature_names = list(X.columns)\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "proved-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dried-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,     0,     0],\n",
       "       [    1,     1,     1, ...,     1,     1,     1],\n",
       "       [    2,     2,     2, ...,     2,     2,     0],\n",
       "       ...,\n",
       "       [   50,     2, 19507, ...,     6,     0,     4],\n",
       "       [   24,     2, 17888, ...,     0,     0,     3],\n",
       "       [    7,     6, 14492, ...,     1,     1,     2]], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "speaking-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "list_feature_names = [str(i) for i in feature_names]\n",
    "feature_names = list_feature_names\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "precise-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.994256933140874\n",
      "Test 0.7371168847122412\n"
     ]
    }
   ],
   "source": [
    "blackbox_model = sklearn.ensemble.RandomForestClassifier(n_estimators=20)\n",
    "blackbox_model.fit(X, y)\n",
    "print('Train', sklearn.metrics.accuracy_score(y, blackbox_model.predict(X)))\n",
    "print('Test', sklearn.metrics.accuracy_score(y_test, blackbox_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "sustained-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "import collections\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import string\n",
    "from io import open\n",
    "import json\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "mature-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  anchor_base.py \n",
    "     https://github.com/marcotcr/anchor/blob/master/anchor/anchor_base.py  \"\"\" \n",
    "from __future__ import print_function\n",
    "# import numpy as np\n",
    "import operator\n",
    "import copy\n",
    "# import sklearn\n",
    "# import collections\n",
    "\n",
    "\"\"\" Used in def get_anchor_from_tuple\"\"\"\n",
    "def matrix_subset(matrix, n_samples):\n",
    "    if matrix.shape[0] == 0:\n",
    "        return matrix\n",
    "    n_samples = min(matrix.shape[0], n_samples)\n",
    "    return matrix[np.random.choice(matrix.shape[0], n_samples, replace=False)]\n",
    "\n",
    "\"\"\" ~~~~ Used in calss AnchorTabularExplainer ~~~~ \"\"\"\n",
    "class AnchorBaseBeam(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" KL divergence between two multivariate Bernoulli \"\"\"\n",
    "    \n",
    "    \"\"\" Used in dlow_bernoulli \"\"\"\n",
    "    \"\"\"The log likelihood ratio can be interpreted as the amount of evidence the data provide for one model versus another, \n",
    "        so the KL divergence tells us how much evidence we can expect our data to provide in favor of the true model.\"\"\"\n",
    "    \n",
    "    \"\"\" D(P||D) = It a measure of how one probability distribution (P) is different from a second (Q)\"\"\"\n",
    "    \"\"\" It is also called relative entropy \"\"\"\n",
    "    \"\"\" It is not the distance between two distribution- often misundestood \"\"\"\n",
    "    \"\"\" p and q are the probability distributions \"\"\"\n",
    "    \"\"\" p to q !=  q to p -> nonsymmetric \"\"\"\n",
    "    \"\"\" D(P||D) = is the information gain when distribution Q is used insted of distibution P \"\"\"\n",
    "    @staticmethod\n",
    "    def kl_bernoulli(p, q):\n",
    "        p = min(0.9999999999999999, max(0.0000001, p))\n",
    "        q = min(0.9999999999999999, max(0.0000001, q))\n",
    "        return (p * np.log(float(p) / q) + (1 - p) *\n",
    "                np.log(float(1 - p) / (1 - q)))\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def compute_beta(n_features, t, delta):\n",
    "        alpha = 1.1\n",
    "        k = 405.5\n",
    "        temp = np.log(k * n_features * (t ** alpha) / delta)\n",
    "        return temp + np.log(temp)\n",
    "    \n",
    "    \"\"\" Used in lucb and anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def dup_bernoulli(p, level):\n",
    "        lm = p\n",
    "        um = min(min(1, p + np.sqrt(level / 2.)), 1)\n",
    "        qm = (um + lm) / 2.\n",
    "#         print 'lm', lm, 'qm', qm, kl_bernoulli(p, qm)\n",
    "        if AnchorBaseBeam.kl_bernoulli(p, qm) > level:\n",
    "            um = qm\n",
    "        else:\n",
    "            lm = qm\n",
    "        return um\n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def lucb(sample_fns, initial_stats, epsilon, delta, batch_size, top_n,\n",
    "             verbose=False, verbose_every=1):\n",
    "        # initial_stats must have n_samples, positive\n",
    "        n_features = len(sample_fns)\n",
    "        n_samples = np.array(initial_stats['n_samples'])\n",
    "        positives = np.array(initial_stats['positives'])\n",
    "        ub = np.zeros(n_samples.shape)\n",
    "        lb = np.zeros(n_samples.shape)\n",
    "        for f in np.where(n_samples == 0)[0]:\n",
    "            n_samples[f] += 1\n",
    "            positives[f] += sample_fns[f](1)\n",
    "        if n_features == top_n:\n",
    "            return range(n_features)\n",
    "        means = positives / n_samples\n",
    "        t = 1\n",
    "\n",
    "        def update_bounds(t):\n",
    "            sorted_means = np.argsort(means)\n",
    "            beta = AnchorBaseBeam.compute_beta(n_features, t, delta)\n",
    "            J = sorted_means[-top_n:]\n",
    "            not_J = sorted_means[:-top_n]\n",
    "            for f in not_J:\n",
    "                ub[f] = AnchorBaseBeam.dup_bernoulli(means[f], beta /\n",
    "                                                     n_samples[f])\n",
    "            for f in J:\n",
    "                lb[f] = AnchorBaseBeam.dlow_bernoulli(means[f],\n",
    "                                                      beta / n_samples[f])\n",
    "            ut = not_J[np.argmax(ub[not_J])]\n",
    "            lt = J[np.argmin(lb[J])]\n",
    "            return ut, lt\n",
    "        ut, lt = update_bounds(t)\n",
    "        B = ub[ut] - lb[lt]\n",
    "        verbose_count = 0\n",
    "        while B > epsilon:\n",
    "            verbose_count += 1\n",
    "            if verbose and verbose_count % verbose_every == 0:\n",
    "                print('Best: %d (mean:%.10f, n: %d, lb:%.4f)' %\n",
    "                      (lt, means[lt], n_samples[lt], lb[lt]), end=' ')\n",
    "                print('Worst: %d (mean:%.4f, n: %d, ub:%.4f)' %\n",
    "                      (ut, means[ut], n_samples[ut], ub[ut]), end=' ')\n",
    "                print('B = %.2f' % B)\n",
    "            n_samples[ut] += batch_size\n",
    "            positives[ut] += sample_fns[ut](batch_size)\n",
    "            means[ut] = positives[ut] / n_samples[ut]\n",
    "            n_samples[lt] += batch_size\n",
    "            positives[lt] += sample_fns[lt](batch_size)\n",
    "            means[lt] = positives[lt] / n_samples[lt]\n",
    "            t += 1\n",
    "            ut, lt = update_bounds(t)\n",
    "            B = ub[ut] - lb[lt]\n",
    "        sorted_means = np.argsort(means)\n",
    "        return sorted_means[-top_n:]\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def dlow_bernoulli(p, level):\n",
    "        um = p\n",
    "        lm = max(min(1, p - np.sqrt(level / 2.)), 0)\n",
    "        qm = (um + lm) / 2.\n",
    "#         print 'lm', lm, 'qm', qm, kl_bernoulli(p, qm)\n",
    "        if AnchorBaseBeam.kl_bernoulli(p, qm) > level:\n",
    "            lm = qm\n",
    "        \n",
    "        else:\n",
    "            um = qm\n",
    "        return lm\n",
    "\n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def make_tuples(previous_best, state):\n",
    "        # alters state, computes support for new tuples\n",
    "        normalize_tuple = lambda x: tuple(sorted(set(x)))  # noqa\n",
    "        all_features = range(state['n_features'])\n",
    "        coverage_data = state['coverage_data']\n",
    "        current_idx = state['current_idx']\n",
    "        data = state['data'][:current_idx]\n",
    "        labels = state['labels'][:current_idx]\n",
    "        \n",
    "        \"\"\" len(previous_best) == 0 ->  first try\"\"\"\n",
    "        if len(previous_best) == 0:\n",
    "            tuples = [(x, ) for x in all_features]\n",
    "#             print(\"all_features\", all_features)\n",
    "#             print(\"tuples\", tuples)\n",
    "            for x in tuples:\n",
    "                pres = data[:, x[0]].nonzero()[0]\n",
    "#                 print(\"pres\", pres)\n",
    "#                 print(\"float(len(pres)\", float(len(pres)))\n",
    "#                 print(\"float(labels[pres].sum())\", float(labels[pres].sum()))\n",
    "#                 print(\"x[0]\", x[0])\n",
    "                # NEW\n",
    "                state['t_idx'][x] = set(pres)\n",
    "                state['t_nsamples'][x] = float(len(pres))\n",
    "                state['t_positives'][x] = float(labels[pres].sum())\n",
    "                state['t_order'][x].append(x[0])\n",
    "                # NEW\n",
    "                state['t_coverage_idx'][x] = set(\n",
    "                    coverage_data[:, x[0]].nonzero()[0])\n",
    "                state['t_coverage'][x] = (\n",
    "                    float(len(state['t_coverage_idx'][x])) /\n",
    "                    coverage_data.shape[0])\n",
    "            return tuples\n",
    "#         print(\"\")\n",
    "        new_tuples = set()\n",
    "        for f in all_features:\n",
    "            for t in previous_best:\n",
    "                new_t = normalize_tuple(t + (f, ))\n",
    "                if len(new_t) != len(t) + 1:\n",
    "                    continue\n",
    "                if new_t not in new_tuples:\n",
    "                    new_tuples.add(new_t)\n",
    "                    state['t_order'][new_t] = copy.deepcopy(state['t_order'][t])\n",
    "                    state['t_order'][new_t].append(f)\n",
    "                    state['t_coverage_idx'][new_t] = (\n",
    "                        state['t_coverage_idx'][t].intersection(\n",
    "                            state['t_coverage_idx'][(f,)]))\n",
    "                    state['t_coverage'][new_t] = (\n",
    "                        float(len(state['t_coverage_idx'][new_t])) /\n",
    "                        coverage_data.shape[0])\n",
    "                    t_idx = np.array(list(state['t_idx'][t]))\n",
    "                    t_data = state['data'][t_idx]\n",
    "                    present = np.where(t_data[:, f] == 1)[0]\n",
    "                    state['t_idx'][new_t] = set(t_idx[present])\n",
    "                    idx_list = list(state['t_idx'][new_t])\n",
    "                    state['t_nsamples'][new_t] = float(len(idx_list))\n",
    "                    state['t_positives'][new_t] = np.sum(\n",
    "                        state['labels'][idx_list])\n",
    "        return list(new_tuples)\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_sample_fns(sample_fn, tuples, state):\n",
    "        # each sample fn returns number of positives\n",
    "        sample_fns = []\n",
    "        def complete_sample_fn(t, n):\n",
    "            raw_data, data, labels = sample_fn(list(t), n)\n",
    "            current_idx = state['current_idx']\n",
    "            # idxs = range(state['data'].shape[0], state['data'].shape[0] + n)\n",
    "            idxs = range(current_idx, current_idx + n)\n",
    "            state['t_idx'][t].update(idxs)\n",
    "            state['t_nsamples'][t] += n\n",
    "            state['t_positives'][t] += labels.sum()\n",
    "            state['data'][idxs] = data\n",
    "            state['raw_data'][idxs] = raw_data\n",
    "            state['labels'][idxs] = labels\n",
    "            state['current_idx'] += n\n",
    "            if state['current_idx'] >= state['data'].shape[0] - max(1000, n):\n",
    "                prealloc_size = state['prealloc_size']\n",
    "                current_idx = data.shape[0]\n",
    "                state['data'] = np.vstack(\n",
    "                    (state['data'],\n",
    "                     np.zeros((prealloc_size, data.shape[1]), data.dtype)))\n",
    "                state['raw_data'] = np.vstack(\n",
    "                    (state['raw_data'],\n",
    "                     np.zeros((prealloc_size, raw_data.shape[1]),\n",
    "                              raw_data.dtype)))\n",
    "                state['labels'] = np.hstack(\n",
    "                    (state['labels'],\n",
    "                     np.zeros(prealloc_size, labels.dtype)))\n",
    "            # This can be really slow\n",
    "            # state['data'] = np.vstack((state['data'], data))\n",
    "            # state['raw_data'] = np.vstack((state['raw_data'], raw_data))\n",
    "            # state['labels'] = np.hstack((state['labels'], labels))\n",
    "            return labels.sum()\n",
    "        for t in tuples:\n",
    "            sample_fns.append(lambda n, t=t: complete_sample_fn(t, n))\n",
    "        \n",
    "#         print(\"sample_fns\", sample_fns)\n",
    "        return sample_fns\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_initial_statistics(tuples, state):\n",
    "        stats = {\n",
    "            'n_samples': [],\n",
    "            'positives': []\n",
    "        }\n",
    "        for t in tuples:\n",
    "            stats['n_samples'].append(state['t_nsamples'][t])\n",
    "            stats['positives'].append(state['t_positives'][t])\n",
    "        return stats\n",
    "    \n",
    "    \"\"\" Used in anchor_beam \"\"\"\n",
    "    @staticmethod\n",
    "    def get_anchor_from_tuple(t, state):\n",
    "        # TODO: This is wrong, some of the intermediate anchors may not exist.\n",
    "        anchor = {'feature': [], 'mean': [], 'precision': [],\n",
    "                  'coverage': [], 'examples': [], 'all_precision': 0}\n",
    "        anchor['num_preds'] = state['data'].shape[0]\n",
    "        normalize_tuple = lambda x: tuple(sorted(set(x)))  # noqa\n",
    "        current_t = tuple()\n",
    "        for f in state['t_order'][t]:\n",
    "            current_t = normalize_tuple(current_t + (f,))\n",
    "\n",
    "            mean = (state['t_positives'][current_t] /\n",
    "                    state['t_nsamples'][current_t])\n",
    "            anchor['feature'].append(f)\n",
    "            anchor['mean'].append(mean)\n",
    "            anchor['precision'].append(mean)\n",
    "            anchor['coverage'].append(state['t_coverage'][current_t])\n",
    "            raw_idx = list(state['t_idx'][current_t])\n",
    "            raw_data = state['raw_data'][raw_idx]\n",
    "            covered_true = (\n",
    "                state['raw_data'][raw_idx][state['labels'][raw_idx] == 1])\n",
    "            covered_false = (\n",
    "                state['raw_data'][raw_idx][state['labels'][raw_idx] == 0])\n",
    "            exs = {}\n",
    "            exs['covered'] = matrix_subset(raw_data, 10)\n",
    "            exs['covered_true'] = matrix_subset(covered_true, 10)\n",
    "            exs['covered_false'] = matrix_subset(covered_false, 10)\n",
    "            exs['uncovered_true'] = np.array([])\n",
    "            exs['uncovered_false'] = np.array([])\n",
    "            anchor['examples'].append(exs)\n",
    "        return anchor\n",
    "    \n",
    "    \"\"\" I think is where the best anchor is selected (using mult)\"\"\"\n",
    "    @staticmethod\n",
    "    def anchor_beam(sample_fn, delta=0.05, epsilon=0.1, batch_size=10,\n",
    "                    min_shared_samples=0, desired_confidence=1, beam_size=1,\n",
    "                    verbose=False, epsilon_stop=0.05, min_samples_start=0,\n",
    "                    max_anchor_size=None, verbose_every=1,\n",
    "                    stop_on_first=False, coverage_samples=10000):\n",
    "        anchor = {'feature': [], 'mean': [], 'precision': [],\n",
    "                  'coverage': [], 'examples': [], 'all_precision': 0}\n",
    "        \n",
    "        \"\"\" sample_fn returns: raw_data, data, labels\n",
    "                raw_data = boostram samples. \n",
    "                data = array of 1 and 0, 1 if the train_dataset is <=, > or == to the rigion... \n",
    "                labels = array of 1 and 0, 1 if the predictions is the same as the true labels, 0 otherwise \"\"\"\n",
    "        \"\"\"coverage_data =  array of 1000 list with 12 elements, each with 0 or 1\"\"\"\n",
    "        _, coverage_data, _ = sample_fn([], coverage_samples, compute_labels=False)\n",
    "        \n",
    "#         print(\"coverage_data\", coverage_data)\n",
    "#         print(\"len(coverage_data)\", len(coverage_data))\n",
    "#         print(\"len(coverage_data[0])\", len(coverage_data[0]))\n",
    "        \n",
    "        \"\"\"max(1, min_samples_start) = max(1, 0) = 1\"\"\"\n",
    "        \"\"\"instance = [] \"\"\"\n",
    "        \"\"\" raw_data = one random boostrap example, data=[1 0 ... 0], labels = predictions\"\"\"\n",
    "        raw_data, data, labels = sample_fn([], max(1, min_samples_start))\n",
    "        \n",
    "#         print(\"raw_data, data, labels\", raw_data, data, labels)\n",
    "        \n",
    "        mean = labels.mean()\n",
    "        beta = np.log(1. / delta)\n",
    "#         print(\"beta / data.shape[0]\", beta / data.shape[0])\n",
    "        lb = AnchorBaseBeam.dlow_bernoulli(mean, beta / data.shape[0])\n",
    "        \n",
    "        \"\"\" desired_confidence = 1, epsilon = 0.1 \"\"\"\n",
    "        while mean > desired_confidence and lb < desired_confidence - epsilon:\n",
    "            \"\"\"batch_size=10 \"\"\"\n",
    "            \"\"\" nraw_data = 10 random boostrap example, ndata=[1 0 ... 0], nlabels = predictions\"\"\"\n",
    "            nraw_data, ndata, nlabels = sample_fn([], batch_size)\n",
    "#             print(\"nraw_data, ndata, nlabels\", nraw_data, ndata, nlabels)\n",
    "            \"\"\" stacks data, row_data and labels, I don't really undestand the point of this\"\"\"\n",
    "            data = np.vstack((data, ndata))\n",
    "            raw_data = np.vstack((raw_data, nraw_data))\n",
    "            raw_data = np.vstack((raw_data, nraw_data))\n",
    "            labels = np.hstack((labels, nlabels))\n",
    "#             print(\"raw_data, data, labels\", raw_data, data, labels)\n",
    "            mean = labels.mean()\n",
    "            lb = AnchorBaseBeam.dlow_bernoulli(mean, beta / data.shape[0])\n",
    "        if lb > desired_confidence:\n",
    "            anchor['num_preds'] = data.shape[0]\n",
    "            anchor['all_precision'] = mean\n",
    "            print(\"if lb > desired_confidence\", lb > desired_confidence)\n",
    "            print(\"anchor['num_preds']\", anchor['num_preds'])\n",
    "            print(\"anchor['all_precision']\", anchor['all_precision'])\n",
    "            return anchor\n",
    "        \n",
    "        \"\"\" batch_size = 10 -> prealloc_size = 100000 \"\"\"\n",
    "        prealloc_size = batch_size * 10000\n",
    "        current_idx = data.shape[0]\n",
    "        data = np.vstack((data, np.zeros((prealloc_size, data.shape[1]),\n",
    "                                         data.dtype)))\n",
    "        raw_data = np.vstack(\n",
    "            (raw_data, np.zeros((prealloc_size, raw_data.shape[1]),\n",
    "                                raw_data.dtype)))\n",
    "        labels = np.hstack((labels, np.zeros(prealloc_size, labels.dtype)))\n",
    "        n_features = data.shape[1]\n",
    "        \n",
    "#         print(\"prealloc_size\", prealloc_size)\n",
    "#         print(\"current_idx\", current_idx)\n",
    "#         print(\"data\", data)\n",
    "#         print(\"raw_data\", raw_data)\n",
    "#         print(\"labels\", labels)\n",
    "#         print(\"n_features\", n_features)\n",
    "        state = {'t_idx': collections.defaultdict(lambda: set()),\n",
    "                 't_nsamples': collections.defaultdict(lambda: 0.),\n",
    "                 't_positives': collections.defaultdict(lambda: 0.),\n",
    "                 'data': data,\n",
    "                 'prealloc_size': prealloc_size,\n",
    "                 'raw_data': raw_data,\n",
    "                 'labels': labels,\n",
    "                 'current_idx': current_idx,\n",
    "                 'n_features': n_features,\n",
    "                 't_coverage_idx': collections.defaultdict(lambda: set()),\n",
    "                 't_coverage': collections.defaultdict(lambda: 0.),\n",
    "                 'coverage_data': coverage_data,\n",
    "                 't_order': collections.defaultdict(lambda: list())\n",
    "                 }\n",
    "        current_size = 1\n",
    "        best_of_size = {0: []}\n",
    "        best_coverage = -1\n",
    "        best_tuple = ()\n",
    "        t = 1\n",
    "        \"\"\" max_anchor_size = None, n_features= 12 (most of the times)\"\"\"\n",
    "        if max_anchor_size is None:\n",
    "            max_anchor_size = n_features\n",
    "            \n",
    "        \"\"\" current_size = 1 \"\"\"\n",
    "        while current_size <= max_anchor_size:\n",
    "            \"\"\" makes tuples from all_features \"\"\"\n",
    "            tuples = AnchorBaseBeam.make_tuples(\n",
    "                best_of_size[current_size - 1], state)\n",
    "#             print(\"\\n tuples before\", tuples)\n",
    "            tuples = [x for x in tuples\n",
    "                      if state['t_coverage'][x] > best_coverage]\n",
    "#             print(\"best_coverage\", best_coverage)\n",
    "#             print(\"tuples after\", tuples, \"\\n\")\n",
    "            \n",
    "            \"\"\" len(tuples) == 0 -> Non of new tubles have a lager coverage compared to the best one found preciously \"\"\"\n",
    "            if len(tuples) == 0:\n",
    "                break\n",
    "            \n",
    "            sample_fns = AnchorBaseBeam.get_sample_fns(sample_fn, tuples,\n",
    "                                                       state)\n",
    "#             print(\"sample_fns\", sample_fns)\n",
    "            initial_stats = AnchorBaseBeam.get_initial_statistics(tuples, state)\n",
    "#             print(\"initial_stats\", initial_stats)\n",
    "            \n",
    "            # print tuples, beam_size\n",
    "            \n",
    "            \"\"\" still yet to undestand how this function works \"\"\"\n",
    "            chosen_tuples = AnchorBaseBeam.lucb(\n",
    "                sample_fns, initial_stats, epsilon, delta, batch_size,\n",
    "                min(beam_size, len(tuples)),\n",
    "                verbose=verbose, verbose_every=verbose_every)\n",
    "            \n",
    "            best_of_size[current_size] = [tuples[x] for x in chosen_tuples]\n",
    "            \n",
    "            \"\"\" Verbose =  False, Verbose that does this mean? \"\"\"\n",
    "            if verbose:\n",
    "                print('Best of size ', current_size, ':')\n",
    "            # print state['data'].shape[0]\n",
    "            stop_this = False\n",
    "            for i, t in zip(chosen_tuples, best_of_size[current_size]):\n",
    "                # I can choose at most (beam_size - 1) tuples at each step,\n",
    "                # and there are at most n_feature steps\n",
    "                \"\"\" why does it chose this beta? \"\"\"\n",
    "                beta = np.log(1. /\n",
    "                              (delta / (1 + (beam_size - 1) * n_features)))\n",
    "                # beta = np.log(1. / delta)\n",
    "                # if state['t_nsamples'][t] == 0:\n",
    "                #     mean = 1\n",
    "                # else:\n",
    "                mean = state['t_positives'][t] / state['t_nsamples'][t]\n",
    "                lb = AnchorBaseBeam.dlow_bernoulli(\n",
    "                    mean, beta / state['t_nsamples'][t])\n",
    "                ub = AnchorBaseBeam.dup_bernoulli(\n",
    "                    mean, beta / state['t_nsamples'][t])\n",
    "                coverage = state['t_coverage'][t]\n",
    "                if verbose:\n",
    "                    print(i, mean, lb, ub)\n",
    "                while ((mean >= desired_confidence and\n",
    "                       lb < desired_confidence - epsilon_stop) or\n",
    "                       (mean < desired_confidence and\n",
    "                        ub >= desired_confidence + epsilon_stop)):\n",
    "                    # print mean, lb, state['t_nsamples'][t]\n",
    "                    sample_fns[i](batch_size)\n",
    "                    mean = state['t_positives'][t] / state['t_nsamples'][t]\n",
    "                    lb = AnchorBaseBeam.dlow_bernoulli(\n",
    "                        mean, beta / state['t_nsamples'][t])\n",
    "                    ub = AnchorBaseBeam.dup_bernoulli(\n",
    "                        mean, beta / state['t_nsamples'][t])\n",
    "                if verbose:\n",
    "                    print('%s mean = %.2f lb = %.2f ub = %.2f coverage: %.2f n: %d' % (t, mean, lb, ub, coverage, state['t_nsamples'][t]))\n",
    "                if mean >= desired_confidence and lb > desired_confidence - epsilon_stop:\n",
    "                    if verbose:\n",
    "                        print('Found eligible anchor ', t, 'Coverage:',\n",
    "                              coverage, 'Is best?', coverage > best_coverage)\n",
    "                    if coverage > best_coverage:\n",
    "                        best_coverage = coverage\n",
    "                        best_tuple = t\n",
    "                        if best_coverage == 1 or stop_on_first:\n",
    "                            stop_this = True\n",
    "            if stop_this:\n",
    "                break\n",
    "            current_size += 1\n",
    "        if best_tuple == ():\n",
    "            # Could not find an anchor, will now choose the highest precision\n",
    "            # amongst the top K from every round\n",
    "            if verbose:\n",
    "                print('Could not find an anchor, now doing best of each size')\n",
    "            tuples = []\n",
    "            for i in range(0, current_size):\n",
    "                tuples.extend(best_of_size[i])\n",
    "            # tuples = best_of_size[current_size - 1]\n",
    "            sample_fns = AnchorBaseBeam.get_sample_fns(sample_fn, tuples,\n",
    "                                                       state)\n",
    "            initial_stats = AnchorBaseBeam.get_initial_statistics(tuples,\n",
    "                                                                  state)\n",
    "            # print tuples, beam_size\n",
    "            chosen_tuples = AnchorBaseBeam.lucb(\n",
    "                sample_fns, initial_stats, epsilon, delta, batch_size,\n",
    "                1, verbose=verbose)\n",
    "            best_tuple = tuples[chosen_tuples[0]]\n",
    "        # return best_tuple, state\n",
    "        return AnchorBaseBeam.get_anchor_from_tuple(best_tuple, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "criminal-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import io\n",
    "\n",
    "\n",
    "\"\"\" ~~~~~ Used in class AnchorBaseBeam(object): ~~~~~ \"\"\"\n",
    "class AnchorExplanation:\n",
    "    \"\"\"Object returned by explainers\"\"\"\n",
    "    def __init__(self, type_, exp_map, as_html):\n",
    "        self.type = type_\n",
    "        self.exp_map = exp_map\n",
    "        self.as_html_fn = as_html\n",
    "    \n",
    "    \"\"\" Not in the main code, just when printting the rages of the anchor's features\"\"\"\n",
    "    def names(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns a list of the names of the anchor conditions.\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor until a certain index.\n",
    "            For example, if the anchor is (A=1,B=2,C=2) and partial_index=1,\n",
    "            this will return [\"A=1\", \"B=2\"]\n",
    "        \"\"\"\n",
    "        names = self.exp_map['names']\n",
    "        if partial_index is not None:\n",
    "            names = names[:partial_index + 1]\n",
    "        return names\n",
    "    \n",
    "    \"\"\" Not in the main code, returns the index of the anchor's features used\"\"\"\n",
    "    def features(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns a list of the features used in the anchor conditions.\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor until a certain index.\n",
    "            For example, if the anchor uses features (1, 2, 3) and\n",
    "            partial_index=1, this will return [1, 2]\n",
    "        \"\"\"\n",
    "        features = self.exp_map['feature']\n",
    "        if partial_index is not None:\n",
    "            features = features[:partial_index + 1]\n",
    "        return features\n",
    "    \n",
    "    \"\"\" Not in the main code, returns precision of the anchor's\"\"\"\n",
    "    def precision(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns the anchor precision (a float)\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor precision until a\n",
    "            certain index. For example, if the anchor has precisions\n",
    "            [0.1, 0.5, 0.95] and partial_index=1, this will return 0.5\n",
    "        \"\"\"\n",
    "        precision = self.exp_map['precision']\n",
    "        if len(precision) == 0:\n",
    "            return self.exp_map['all_precision']\n",
    "        if partial_index is not None:\n",
    "            return precision[partial_index]\n",
    "        else:\n",
    "            return precision[-1]\n",
    "\n",
    "    \"\"\" Not in the main code, returns coverage of the anchor's \"\"\"\n",
    "    def coverage(self, partial_index=None):\n",
    "        \"\"\"\n",
    "        Returns the anchor coverage (a float)\n",
    "        Args:\n",
    "            partial_index (int): lets you get the anchor coverage until a\n",
    "            certain index. For example, if the anchor has coverages\n",
    "            [0.1, 0.5, 0.95] and partial_index=1, this will return 0.5\n",
    "        \"\"\"\n",
    "        coverage = self.exp_map['coverage']\n",
    "        if len(coverage) == 0:\n",
    "            return 1\n",
    "        if partial_index is not None:\n",
    "            return coverage[partial_index]\n",
    "        else:\n",
    "            return coverage[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "hybrid-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" anchor_tabular.py \n",
    "    https://github.com/marcotcr/anchor/blob/master/anchor/anchor_tabular.py\"\"\"\n",
    "\n",
    "\"\"\"def as_html(, def jsonize(x)\"\"\"\n",
    "\n",
    "def id_generator(size=15):\n",
    "    \"\"\"Helper function to generate random div ids. This is useful for embedding\n",
    "    HTML into ipython notebooks.\"\"\"\n",
    "    chars = list(string.ascii_uppercase + string.digits)\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\" This is the main class-file to compute the anchors for tabular data \"\"\"\n",
    "class AnchorTabularExplainer(object):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            class_names: list of strings\n",
    "            feature_names: list of strings\n",
    "            train_data: used to sample (bootstrap)\n",
    "            categorical_names: map from integer to list of strings, names for each\n",
    "                value of the categorical features. Every feature that is not in\n",
    "                this map will be considered as ordinal or continuous, and thus discretized.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, class_names, feature_names, train_data, train_label = None,\n",
    "                 categorical_names={}, discretizer='quartile', encoder_fn=None):\n",
    "        self.min = {}\n",
    "        self.max = {}\n",
    "        self.disc = collections.namedtuple('random_name2',\n",
    "                                              ['discretize'])(lambda x: x)\n",
    "        self.encoder_fn = lambda x: x\n",
    "        if encoder_fn is not None:\n",
    "            self.encoder_fn = encoder_fn\n",
    "        self.categorical_features = []\n",
    "        self.feature_names = feature_names\n",
    "        self.train = train_data\n",
    "        self.class_names = class_names\n",
    "        self.categorical_names = copy.deepcopy(categorical_names)\n",
    "        if categorical_names:\n",
    "            self.categorical_features = sorted(categorical_names.keys())\n",
    "\n",
    "        if discretizer == 'quartile':\n",
    "            self.disc = QuartileDiscretizer(train_data,self.categorical_features,\n",
    "                                                         self.feature_names)\n",
    "        elif discretizer == 'decile':\n",
    "            self.disc = lime.lime_tabular.DecileDiscretizer(train_data,\n",
    "                                                     self.categorical_features,\n",
    "                                                     self.feature_names)\n",
    "        elif discretizer == \"entropy\":\n",
    "            self.disc = lime.lime_tabular.EntropyDiscretizer(train_data,\n",
    "                                                     self.categorical_features,\n",
    "                                                     self.feature_names, train_label)\n",
    "            \n",
    "        elif discretizer == 'GaussianMixtureModels':\n",
    "            self.disc = GaussianMixtureModelsDiscretizer(train_data, self.categorical_features,\n",
    "                                                         self.feature_names, train_label)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Discretizer must be quartile, decile or GaussianMixtureModels')\n",
    "\n",
    "        \"\"\" ?? \"\"\"\n",
    "        self.ordinal_features = [x for x in range(len(feature_names)) if x not in self.categorical_features]\n",
    "        \n",
    "#         print(\"self.train\", self.train)\n",
    "#         self.d_train = self.disc.discretize(self.train)\n",
    "        self.d_train = self.disc.discretize(self.train)\n",
    "#         print(\"self.d_train\", self.d_train)\n",
    "#         print(\"self.d_train.type()\", type(self.d_train))\n",
    "        self.categorical_names.update(self.disc.names)\n",
    "#         print(\"self.categorical_names\", self.categorical_names)\n",
    "        self.categorical_features += self.ordinal_features\n",
    "#         print(\"self.categorical_features\", self.categorical_features)\n",
    "\n",
    "        \"\"\" selects the min and max in the training data set\"\"\"\n",
    "        for f in range(train_data.shape[1]):\n",
    "            self.min[f] = np.min(train_data[:, f])\n",
    "            self.max[f] = np.max(train_data[:, f])\n",
    "            \n",
    "    \n",
    "    \"\"\" This is def explain_instance: def sample_fn \"\"\"\n",
    "    \"\"\"Randomelly selects with replacement from the training dataset where feature is > the saved reagio\n",
    "                     and samples as many samples from the boostrap <=  to the saved reagion \"\"\"\n",
    "    def sample_from_train(self, conditions_eq, conditions_neq, conditions_geq,\n",
    "                          conditions_leq, num_samples):\n",
    "        \"\"\"\n",
    "        bla\n",
    "        \"\"\"\n",
    "        \"\"\" Train the list of training input dataset, d_train is the discretized training input datset.\"\"\"\n",
    "        train = self.train\n",
    "        d_train = self.d_train\n",
    "        \n",
    "        \"\"\"train.shape[0]=len(train)=120, , with replacement --> boostrap, \n",
    "            selects n (= num_samples) indexes from the train dataset with replacement\"\"\"\n",
    "        idx = np.random.choice(range(train.shape[0]), num_samples, replace=True)\n",
    "        \n",
    "        \"\"\" selectes multiple samples with replacement \"\"\"\n",
    "        sample = train[idx] # sample = the selected train from the selected index\n",
    "        d_sample = d_train[idx] # d_sample = sample discretized\n",
    "\n",
    "        \n",
    "        for f in conditions_eq:\n",
    "            sample[:, f] = np.repeat(conditions_eq[f], num_samples)\n",
    "        \n",
    "\n",
    "        \n",
    "        \"\"\"conditions_geq[f] = the rigion of the saved from the quartile with the '<='' or ''>',\n",
    "            f is the location saved in conditions_geq\"\"\"\n",
    "        for f in conditions_geq:\n",
    "#             print(\"f\", f)\n",
    "#             print(\"conditions_geq\", conditions_geq)\n",
    "#             print(\"conditions_geq[f]\", conditions_geq[f])\n",
    "#             print(\"d_sample[:, f]\", d_sample[:, f])\n",
    "            \"\"\" d_sample[:, f] = all of the discretized sample in the f location (index).\"\"\"\n",
    "            \"\"\" idx = for all the d_sample checks in the f index is <= the saved region (conditions_geq[f]),\n",
    "                returning Ture or False. \"\"\"\n",
    "            idx = d_sample[:, f] <= conditions_geq[f]\n",
    "#             print('len(idx)', len(idx))\n",
    "#             print('len(d_sample[:, f])', len(d_sample[:, f]))\n",
    "#             print(\"\\nif f in conditions_leq\", f in conditions_leq)\n",
    "            \n",
    "            if f in conditions_leq:\n",
    "                idx = (idx + (d_sample[:, f] > conditions_leq[f])).astype(bool)\n",
    "            \n",
    "            \"\"\" Cheeks if there all False \"\"\"\n",
    "            \"\"\" continue... What is the point of this?\"\"\"\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "#             print(\"\\nf\", f)\n",
    "#             print(\"d_train\", d_train)\n",
    "#             print(\"d_train[:, f]\", d_train[:, f])\n",
    "#             print(\"conditions_geq\", conditions_geq)\n",
    "#             print(\"conditions_geq[f]\", conditions_geq[f])\n",
    "            \"\"\" options  =  for all the d_train checks in the f index is > the saved region (conditions_geq[f]),\n",
    "                returning Ture or False. Oposite to idex\"\"\"\n",
    "            options = d_train[:, f] > conditions_geq[f]\n",
    "#             print(\"float(conditions_geq[f])\", float(conditions_geq[f]))\n",
    "#             print(\"options = d_train[:, f] > conditions_geq[f]\", options)\n",
    "\n",
    "            if f in conditions_leq:\n",
    "                options = options * (d_train[:, f] <= conditions_leq[f])\n",
    "            \n",
    "#             print('self.min[f]', self.min[f])\n",
    "#             print('self.max[f]', self.max[f])\n",
    "            \"\"\" if options.sum() -> options=[ False ... Fase] -> all of the regions on the sample are < conditions_geq[f] \"\"\"\n",
    "            if options.sum() == 0:\n",
    "                min_ = conditions_geq.get(f, self.min[f])\n",
    "                max_ = conditions_leq.get(f, self.max[f])\n",
    "                to_rep = np.random.uniform(min_, max_, idx.sum())\n",
    "            else:\n",
    "                \"\"\" options  = for all the d_train checks in the f index is > the saved region (conditions_geq[f])\n",
    "                    returning Ture or False.\n",
    "                    \n",
    "                    idx.sum() =  number of True in idx/\n",
    "                    \n",
    "                    idx = from the sample checks in the f index is <= the saved region (conditions_geq[f]),\n",
    "                    returning Ture or False. Opposite to options.\n",
    "                    \n",
    "                    reaplace = True -> with replecemet -> Features can be selected multiple times -> bootstrapping. \n",
    "                    \n",
    "                    len(train[options, f]) == options.sum() & idx.sum() == len(to_rep)\n",
    "                    \n",
    "                    \"\"\"\n",
    "                \"\"\" Randomelly selects with replacement from the training dataset where feature is > the saved reagio\n",
    "                     and samples as many samples from the boostrap <=  to the saved reagion \"\"\"\n",
    "                \n",
    "                \"\"\" So we are sampling fron the training dataset only if the condtion is grater, and sampling as many\n",
    "                    times as in the sample it was <=. why? I would have thought if we want to be representative of the sample\n",
    "                    we would have sample as manites they are >  not <= \"\"\"\n",
    "                to_rep = np.random.choice(train[options, f], idx.sum(), replace=True)\n",
    "#                 print('train[options, f]', train[options, f])\n",
    "#                 print('len(train[options, f])', len(train[options, f]))\n",
    "#                 print('options.sum()', options.sum())\n",
    "#                 print('idx.sum()', idx.sum())\n",
    "#                 print('len(to_rep)', len(to_rep))\n",
    "#                 print('to_rep', to_rep)\n",
    "#                 print('len(train)', len(train))\n",
    "        \n",
    "#                 print(\"\\n len(train)\", len(train))\n",
    "#                 print(\"options.sum() + idx.sum()\", options.sum() + idx.sum())\n",
    "                    \n",
    "#                 options_2 = d_train[:, f] > conditions_geq[f]\n",
    "#                 idx_2 = d_sample[:, f] <= conditions_geq[f]\n",
    "                \n",
    "#                 print(\"len(d_train[:, f])\", len(d_train[:, f]))\n",
    "#                 print(\"len(idx)\", len(idx))\n",
    "#                 print(\"len(options)\", len(options))\n",
    "#                  if options.all() == options_2.all() & idx.all() == idx_2.all():\n",
    "#                     print(\"\\nYessssss\")\n",
    "#                     print(\"options_2 = d_train[:, f] > conditions_geq[f], idx_2 = idx = d_sample[:, f] <= conditions_geq[f]\")\n",
    "#                     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#                     print(\"conditions_geq[f]\", conditions_geq[f])\n",
    "#                     print(\"d_train[:, f]\", d_train[:, f])\n",
    "#                     print(\"options\", options)\n",
    "#                     print(\"idx\", idx)\n",
    "#                     print(\"len(d_sample[:, f])\", len(d_sample[:, f]))\n",
    "#                     print(\"options.sum()\", options.sum())\n",
    "#                     print(\"idx.sum\", idx.sum())\n",
    "#                     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "#                 else:\n",
    "#                     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#                     print(\"\\nNOOOOOO\")\n",
    "#                     print(\"conditions_geq[f]\", conditions_geq[f])\n",
    "#                     print(\"d_train[:, f]\", d_train[:, f])\n",
    "#                     print(\"options\", options)\n",
    "#                     print(\"idx\", idx)\n",
    "#                     print(\"len(d_sample[:, f])\", len(d_sample[:, f]))\n",
    "#                     print(\"options.sum()\", options.sum())\n",
    "#                     print(\"idx.sum\", idx.sum())\n",
    "#                     print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "                \n",
    "            sample[idx, f] = to_rep\n",
    "        \n",
    "        \"\"\" Same thing as before but with conditions_leq \"\"\"\n",
    "        for f in conditions_leq:\n",
    "            if f in conditions_geq:\n",
    "                continue\n",
    "            idx = d_sample[:, f] > conditions_leq[f]\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            options = d_train[:, f] <= conditions_leq[f]\n",
    "            if options.sum() == 0:\n",
    "                min_ = conditions_geq.get(f, self.min[f])\n",
    "                max_ = conditions_leq.get(f, self.max[f])\n",
    "                to_rep = np.random.uniform(min_, max_, idx.sum())\n",
    "            else:\n",
    "                to_rep = np.random.choice(train[options, f], idx.sum(),\n",
    "                                          replace=True)\n",
    "            sample[idx, f] = to_rep\n",
    "        \n",
    "#         print(\"sample\", sample)\n",
    "        return sample\n",
    "\n",
    "    \"\"\" Used in get_sample_fn: def sample_fn \"\"\"\n",
    "    def as_html(self, exp, **kwargs):\n",
    "        \"\"\"bla\"\"\"\n",
    "        exp_map = self.to_explanation_map(exp)\n",
    "\n",
    "        def jsonize(x): return json.dumps(x)\n",
    "        this_dir, _ = os.path.split(__file__)\n",
    "        bundle = open(os.path.join(this_dir, 'bundle.js'), encoding='utf8').read()\n",
    "        random_id = 'top_div' + id_generator()\n",
    "        out = u'''<html>\n",
    "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n",
    "        <head><script>%s </script></head><body>''' % bundle\n",
    "        out += u'''\n",
    "        <div id=\"{random_id}\" />\n",
    "        <script>\n",
    "            div = d3.select(\"#{random_id}\");\n",
    "            lime.RenderExplanationFrame(div,{label_names}, {predict_proba},\n",
    "            {true_class}, {explanation}, {raw_data}, \"tabular\", {explanation_type});\n",
    "        </script>'''.format(random_id=random_id,\n",
    "                            label_names=jsonize(exp_map['labelNames']),\n",
    "                            predict_proba=jsonize(exp_map['predictProba']),\n",
    "                            true_class=jsonize(exp_map['trueClass']),\n",
    "                            explanation=jsonize(exp_map['explanation']),\n",
    "                            raw_data=jsonize(exp_map['rawData']),\n",
    "                            explanation_type=jsonize(exp_map['explanationType']))\n",
    "        out += u'</body></html>'\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \"\"\" Used in get_sample_fn: def sample_fn \"\"\"\n",
    "    def as_html(self, exp, **kwargs):\n",
    "        \"\"\"bla\"\"\"\n",
    "        exp_map = self.to_explanation_map(exp)\n",
    "\n",
    "#         def jsonize(x): return json.dumps(x)\n",
    "#         this_dir, _ = os.path.split(__file__)\n",
    "#         bundle = open(os.path.join(this_dir, 'bundle.js'), encoding='utf8').read()\n",
    "#         random_id = 'top_div' + id_generator()\n",
    "#         out = u'''<html>\n",
    "#         <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF8\">\n",
    "#         <head><script>%s </script></head><body>''' % bundle\n",
    "#         out += u'''\n",
    "#         <div id=\"{random_id}\" />\n",
    "#         <script>\n",
    "#             div = d3.select(\"#{random_id}\");\n",
    "#             lime.RenderExplanationFrame(div,{label_names}, {predict_proba},\n",
    "#             {true_class}, {explanation}, {raw_data}, \"tabular\", {explanation_type});\n",
    "#         </script>'''.format(random_id=random_id,\n",
    "#                             label_names=jsonize(exp_map['labelNames']),\n",
    "#                             predict_proba=jsonize(exp_map['predictProba']),\n",
    "#                             true_class=jsonize(exp_map['trueClass']),\n",
    "#                             explanation=jsonize(exp_map['explanation']),\n",
    "#                             raw_data=jsonize(exp_map['rawData']),\n",
    "#                             explanation_type=jsonize(exp_map['explanationType']))\n",
    "#         out += u'</body></html>'\n",
    "        return out\n",
    "    \n",
    "    \"\"\" Used in def explain_instance \"\"\"\n",
    "    \n",
    "    \"\"\" This function returns: sample_fn, mapping\n",
    "             mapping = makes a map dictionary saving if checking from 0 to 3 if the location of the quartile is <= or > \n",
    "             otherwise, for each of the features.\n",
    "             sample_fn = function \n",
    "             \"\"\"\n",
    "    \"\"\" sample_fn returns: raw_data, data, labels,\n",
    "            raw_data = samples \n",
    "            data = array of 1 and 0, 1 if the train_dataset is <=, > or == to the rigion...\n",
    "            labels = array of 1 and 0, 1 if the predictions is the same as the true labels, 0 otherwise \"\"\"\n",
    "    def get_sample_fn(self, data_row, classifier_fn, desired_label=None):\n",
    "        \n",
    "        \"\"\" data_row = the instance we are interested \"\"\"\n",
    "        \"\"\"predict_fn(x) its the prediction of the blackbox model\"\"\"\n",
    "        def predict_fn(x):\n",
    "            return classifier_fn(self.encoder_fn(x))\n",
    "        true_label = desired_label\n",
    "        if true_label is None:\n",
    "            true_label = predict_fn(data_row.reshape(1, -1))[0]\n",
    "#             print(\"data_row\", data_row)\n",
    "#             print(\"data_row.reshape(1, -1)\", data_row.reshape(1, -1)[0])\n",
    "#             print(\"true_label in loop\", true_label)\n",
    "        # must map present here to include categorical features (for conditions_eq), and numerical features for geq and leq\n",
    "        \"\"\" makes a map dictionary saving if checking from 0 to 3 if the location of the quartile is <= or > otherwise,\n",
    "            for each of the features\"\"\"\n",
    "        mapping = {}\n",
    "        \"\"\" Discretize the features into quartiles \"\"\"\n",
    "        data_row = self.disc.discretize(data_row.reshape(1, -1))[0]\n",
    "        \n",
    "#         print(\"data_row\", data_row)\n",
    "        \n",
    "        \"\"\"self.categorical_feature = self.ordinal_features: are list of the number of (n) features\n",
    "            from 0 to n-1: [0, 1, 2, 3],  v goes from 0 to 3, v is the region of in the quartile domain\"\"\"\n",
    "        \"\"\" I think I could chage the code to something more efficient\"\"\"\n",
    "        for f in self.categorical_features:\n",
    "            if f in self.ordinal_features:\n",
    "#                 print(\"f\", f)\n",
    "#                 print(\"len(self.categorical_names[f])\", len(self.categorical_names[f]))\n",
    "                for v in range(len(self.categorical_names[f])):\n",
    "                    idx = len(mapping)\n",
    "# #                     print(\"mapping:\", mapping)\n",
    "# #                     print(\"idx = len(mapping):\",  idx)\n",
    "# #                     print(\"\\n f, v:\", f, v)\n",
    "# #                     print(\"data_row:\",  data_row)\n",
    "# #                     print(\"data_row[f]:\",  data_row[f])\n",
    "# #                     print(\"data_row[f] <= v\", data_row[f] <= v)\n",
    "# #                     print(\"v != len(self.categorical_names[f]) - 1:\",  v != len(self.categorical_names[f]) - 1)\n",
    "                    if data_row[f] <= v and v != len(self.categorical_names[f]) - 1:\n",
    "                        mapping[idx] = (f, 'leq', v)\n",
    "                        # names[idx] = '%s <= %s' % (self.feature_names[f], v)\n",
    "                    elif data_row[f] > v:\n",
    "                        mapping[idx] = (f, 'geq', v)\n",
    "                        # names[idx] = '%s > %s' % (self.feature_names[f], v)\n",
    "            else:\n",
    "#                 print(\"RICHARD THINKS I WILL NEVER BE PRINTED SINCE F WILL ALWAYS BE IN self.ordinal_features\")\n",
    "                idx = len(mapping)\n",
    "                mapping[idx] = (f, 'eq', data_row[f])\n",
    "            # names[idx] = '%s = %s' % (\n",
    "            #     self.feature_names[f],\n",
    "            #     self.categorical_names[f][int(data_row[f])])\n",
    "        \n",
    "        \"\"\" This function returns raw_data, data, labels,\n",
    "            raw_data = samples \n",
    "            data = array of 1 and 0, 1 if the train_dataset is <=, > or == to the rigion...\n",
    "            labels = array of 1 and 0, 1 if the predictions is the same as the true labels, 0 otherwise \"\"\"\n",
    "        def sample_fn(present, num_samples, compute_labels=True):\n",
    "            conditions_eq = {}\n",
    "            conditions_leq = {}\n",
    "            conditions_geq = {}\n",
    "            \n",
    "#             print(\"present:\", present)\n",
    "            \n",
    "            \"\"\" I think is the idex of the instance\"\"\"\n",
    "            for x in present:\n",
    "                f, op, v = mapping[x]\n",
    "                if op == 'eq':\n",
    "                    conditions_eq[f] = float(v)\n",
    "                if op == 'leq':\n",
    "                    if f not in conditions_leq:\n",
    "                        conditions_leq[f] = v\n",
    "                    conditions_leq[f] = min(conditions_leq[f], float(v))\n",
    "                if op == 'geq':\n",
    "                    if f not in conditions_geq:\n",
    "                        conditions_geq[f] = v\n",
    "                    conditions_geq[f] = max(conditions_geq[f], float(v))\n",
    "            # conditions_eq = dict([(x, data_row[x]) for x in present])\n",
    "#             print(\"conditions_eq\", conditions_eq)\n",
    "#             print(\"conditions_leq\", conditions_leq)\n",
    "#             print(\"conditions_geq\", conditions_geq)\n",
    "\n",
    "            \"\"\"sample_from_train: \n",
    "                    Randomelly selects with replacement from the training dataset where feature is > the saved reagio\n",
    "                    and samples as many samples from the boostrap <=  to the saved reagion, and vice versa\"\"\"\n",
    "            raw_data = self.sample_from_train(\n",
    "                conditions_eq, {}, conditions_geq, conditions_leq, num_samples)\n",
    "            d_raw_data = self.disc.discretize(raw_data)\n",
    "#             print(\"raw_data\", raw_data)\n",
    "#             print(\"d_raw_data\", d_raw_data)\n",
    "            data = np.zeros((num_samples, len(mapping)), int)\n",
    "    \n",
    "\n",
    "        \n",
    "            \"\"\" Checks if the are in the same region, returns 1 or 0 if True or False\n",
    "                why is this for? do we get an array of onlly 1 at the end? \"\"\"\n",
    "            \"\"\" len(len(mapping)) == 12 since 3*4 from the fucntions above wiouth 0?\"\"\"\n",
    "#             print(\"len(mapping)\", len(mapping))\n",
    "            for i in mapping:\n",
    "                f, op, v = mapping[i]\n",
    "#                 print(\"len(d_raw_data[:, f])\", len(d_raw_data[:, f]))\n",
    "#                 print(\"data_row[f]\", data_row[f])\n",
    "                if op == 'eq':\n",
    "                    data[:, i] = (d_raw_data[:, f] == data_row[f]).astype(int)\n",
    "                if op == 'leq':\n",
    "                    data[:, i] = (d_raw_data[:, f] <= v).astype(int)\n",
    "                if op == 'geq':\n",
    "                    data[:, i] = (d_raw_data[:, f] > v).astype(int)\n",
    "            # data = (raw_data == data_row).astype(int)\n",
    "            labels = []\n",
    "            \n",
    "            \"\"\" Compare if the predictions are correct of the instance\"\"\"\n",
    "            \"\"\" However are we not comparing the prediction against the predictino since no true label was given \n",
    "                since true_label  was computed using the blackbox model \"\"\"\n",
    "            if compute_labels:\n",
    "                labels = (predict_fn(raw_data) == true_label).astype(int)\n",
    "            \n",
    "#             print(\"labels\", labels)\n",
    "            return raw_data, data, labels\n",
    "        \n",
    "#         print(\"mapping\", mapping)\n",
    "        return sample_fn, mapping\n",
    "         \n",
    "    \"\"\" Used outside to explain an instance, this is the core functin, where all of the explanabily can happends ****\"\"\"\n",
    "    def explain_instance(self, data_row, classifier_fn, threshold=0.95,\n",
    "                          delta=0.1, tau=0.15, batch_size=100,\n",
    "                          max_anchor_size=None,\n",
    "                          desired_label=None,\n",
    "                          beam_size=4, **kwargs):\n",
    "        # It's possible to pass in max_anchor_size\n",
    "        \n",
    "        \"\"\" This function returns: sample_fn, mapping\n",
    "        \n",
    "                 mapping = makes a map dictionary saving if checking from 0 to 3 if the location of the quartile is <= or > \n",
    "                 otherwise, for each of the features.\n",
    "\n",
    "                 sample_fn = a function \n",
    "         \"\"\"\n",
    "        sample_fn, mapping = self.get_sample_fn(\n",
    "            data_row, classifier_fn, desired_label=desired_label)\n",
    "        # return sample_fn, mapping\n",
    "        \n",
    "        \n",
    "        \"\"\" Changing exp = anchor_base.AnchorBaseBeam.anchor_beam(...) to exp = AnchorBaseBeam.anchor_beam(...) \"\"\"\n",
    "        \"\"\" sample_fn returns raw_data, data, labels,\n",
    "                raw_data = samples \n",
    "                data = array of 1 and 0, 1 if the train_dataset is <=, > or == to the rigion...\n",
    "                labels = array of 1 and 0, 1 if the predictions is the same as the true labels, 0 otherwise \"\"\"\n",
    "        exp = AnchorBaseBeam.anchor_beam(\n",
    "            sample_fn, delta=delta, epsilon=tau, batch_size=batch_size,\n",
    "            desired_confidence=threshold, max_anchor_size=max_anchor_size,\n",
    "            **kwargs)\n",
    "        self.add_names_to_exp(data_row, exp, mapping)\n",
    "        exp['instance'] = data_row\n",
    "        exp['prediction'] = classifier_fn(self.encoder_fn(data_row.reshape(1, -1)))[0]\n",
    "        \n",
    "        \"\"\" cahnge: explanation = anchor_explanation.AnchorExplanation('tabular', exp, self.as_html)\n",
    "                to: explanation = AnchorExplanation('tabular', exp, self.as_html) \n",
    "                Not imported but class defined \"\"\"\n",
    "        explanation = AnchorExplanation('tabular', exp, self.as_html)\n",
    "        return explanation\n",
    "    \n",
    "    \"\"\" Used in explain_instance \"\"\"\n",
    "    def add_names_to_exp(self, data_row, hoeffding_exp, mapping):\n",
    "        # TODO: precision recall is all wrong, coverage functions wont work\n",
    "        # anymore due to ranges\n",
    "        idxs = hoeffding_exp['feature']\n",
    "        hoeffding_exp['names'] = []\n",
    "        hoeffding_exp['feature'] = [mapping[idx][0] for idx in idxs]\n",
    "        ordinal_ranges = {}\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            if op == 'geq' or op == 'leq':\n",
    "                if f not in ordinal_ranges:\n",
    "                    ordinal_ranges[f] = [float('-inf'), float('inf')]\n",
    "            if op == 'geq':\n",
    "                ordinal_ranges[f][0] = max(ordinal_ranges[f][0], v)\n",
    "            if op == 'leq':\n",
    "                ordinal_ranges[f][1] = min(ordinal_ranges[f][1], v)\n",
    "        handled = set()\n",
    "        for idx in idxs:\n",
    "            f, op, v = mapping[idx]\n",
    "            # v = data_row[f]\n",
    "            if op == 'eq':\n",
    "                fname = '%s = ' % self.feature_names[f]\n",
    "                if f in self.categorical_names:\n",
    "                    v = int(v)\n",
    "                    if ('<' in self.categorical_names[f][v]\n",
    "                            or '>' in self.categorical_names[f][v]):\n",
    "                        fname = ''\n",
    "                    fname = '%s%s' % (fname, self.categorical_names[f][v])\n",
    "                else:\n",
    "                    fname = '%s%.2f' % (fname, v)\n",
    "            else:\n",
    "                if f in handled:\n",
    "                    continue\n",
    "                geq, leq = ordinal_ranges[f]\n",
    "                fname = ''\n",
    "                geq_val = ''\n",
    "                leq_val = ''\n",
    "                if geq > float('-inf'):\n",
    "                    if geq == len(self.categorical_names[f]) - 1:\n",
    "                        geq = geq - 1\n",
    "                    name = self.categorical_names[f][geq + 1]\n",
    "                    if '<' in name:\n",
    "                        geq_val = name.split()[0]\n",
    "                    elif '>' in name:\n",
    "                        geq_val = name.split()[-1]\n",
    "                if leq < float('inf'):\n",
    "                    name = self.categorical_names[f][leq]\n",
    "                    if leq == 0:\n",
    "                        leq_val = name.split()[-1]\n",
    "                    elif '<' in name:\n",
    "                        leq_val = name.split()[-1]\n",
    "                if leq_val and geq_val:\n",
    "                    fname = '%s < %s <= %s' % (geq_val, self.feature_names[f],\n",
    "                                               leq_val)\n",
    "                elif leq_val:\n",
    "                    fname = '%s <= %s' % (self.feature_names[f], leq_val)\n",
    "                elif geq_val:\n",
    "                    fname = '%s > %s' % (self.feature_names[f], geq_val)\n",
    "                handled.add(f)\n",
    "            hoeffding_exp['names'].append(fname)\n",
    "   \n",
    "#     \"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  NOT NEEDED  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
    "#     \"\"\" This functions in not used at all in the code \"\"\"\n",
    "#     def transform_to_examples(self, examples, features_in_anchor=[],\n",
    "#                               predicted_label=None):\n",
    "#         ret_obj = []\n",
    "#         if len(examples) == 0:\n",
    "#             return ret_obj\n",
    "#         weights = [int(predicted_label) if x in features_in_anchor else -1\n",
    "#                    for x in range(examples.shape[1])]\n",
    "#         examples = self.disc.discretize(examples)\n",
    "#         for ex in examples:\n",
    "#             values = [self.categorical_names[i][int(ex[i])]\n",
    "#                       if i in self.categorical_features\n",
    "#                       else ex[i] for i in range(ex.shape[0])]\n",
    "#             ret_obj.append(list(zip(self.feature_names, values, weights)))\n",
    "#         return ret_obj\n",
    "    \n",
    "\n",
    "#     \"\"\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ END - NOT NEEDED  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\"\n",
    "\n",
    "# # \"\"\" We do not use  def transform_to_examples and  def to_explanation_map(self, exp)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "entire-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuartileDiscretizer(BaseDiscretizer):\n",
    "    def __init__(self, data, categorical_features, feature_names, labels=None, random_state=None):\n",
    "\n",
    "        BaseDiscretizer.__init__(self, data, categorical_features,\n",
    "                                 feature_names, labels=labels,\n",
    "                                 random_state=random_state)\n",
    "\n",
    "    def bins(self, data, labels):\n",
    "        bins = []\n",
    "        for feature in self.to_discretize:\n",
    "            qts = np.array(np.percentile(data[:, feature], [25, 50, 75]))\n",
    "            bins.append(qts)\n",
    "        return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "decent-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(X)[:, 0:8]\n",
    "labels_train = np.array(y)\n",
    "\n",
    "test = np.array(X_test)[:, 0:8]\n",
    "labels_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aquatic-stability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "affected-harris",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    }
   ],
   "source": [
    "explainer_1 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'quartile')\n",
    "\n",
    "explainer_2 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'decile')\n",
    "\n",
    "explainer_3 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'entropy')\n",
    "\n",
    "explainer_4 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'GaussianMixtureModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "seeing-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Number\" : [], \"Name\": [], \n",
    "               \"quartile mean precision\": [], \"quartile mean coverage\": [], \n",
    "               \"decile mean precision\": [], \"decile mean coverage\": [],\n",
    "               \"entropy mean precision\": [], \"entropy mean coverage\": [],\n",
    "               \"GaussianMixtureModels mean precision\": [], \"GaussianMixtureModels mean coverage\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "reduced-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "hydraulic-degree",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: 1 <= 0.00 AND 4 > 0.00 AND 3 <= 3.00 AND 6 <= 4.93 AND 0 <= 40.76\n",
      "Precision: 0.95\n",
      "Coverage: 0.00\n"
     ]
    }
   ],
   "source": [
    "exp = explainer_4.explain_instance(test[1], blackbox_model.predict, threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "verbal-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: 0.50 < 3 <= 2.50 AND 0.50 < 0 <= 27.50 AND 1 <= 2.50 AND 0.50 < 4 <= 2.50 AND 2 <= 833.50 AND 7 <= 4.50 AND 6 <= 4.50\n",
      "Precision: 0.91\n",
      "Coverage: 0.00\n"
     ]
    }
   ],
   "source": [
    "exp = explainer_3.explain_instance(test[1], blackbox_model.predict, threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "original-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: 1 <= 1.00 AND 0.00 < 3 <= 2.00 AND 6 <= 4.00 AND 4 <= 8.00 AND 0 <= 32.00\n",
      "Precision: 0.96\n",
      "Coverage: 0.01\n"
     ]
    }
   ],
   "source": [
    "exp = explainer_2.explain_instance(test[1], blackbox_model.predict, threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "retained-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: 3 <= 2.00 AND 1 <= 2.00\n",
      "Precision: 0.82\n",
      "Coverage: 0.43\n"
     ]
    }
   ],
   "source": [
    "exp = explainer_1.explain_instance(test[1], blackbox_model.predict, threshold=0.95)\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-blind",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-activity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "established-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def getMeanPrecisionCoverage(explainer, results, discretizer_name):\n",
    "    test_precision = []\n",
    "    test_coverage = []\n",
    "    for idx in range(len(test)):\n",
    "        exp = explainer.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)\n",
    "        \n",
    "        test_precision.append(exp.precision())\n",
    "        test_coverage.append(exp.coverage())\n",
    "#         print('exp.precision()', exp.precision())\n",
    "#         print('exp.coverage()', exp.coverage())\n",
    "        \n",
    "\n",
    "    results[discretizer_name + \" mean precision\"].append(statistics.mean(test_precision))\n",
    "    results[discretizer_name + \" mean coverage\"].append(statistics.mean(test_coverage))\n",
    "#     print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "#     print('Precision: %.2f' % exp.precision())\n",
    "#     print('Coverage: %.2f' % exp.coverage())\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "equipped-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Number\" : [], \"Name\": [], \n",
    "               \"quartile mean precision\": [], \"quartile mean coverage\": [], \n",
    "               \"decile mean precision\": [], \"decile mean coverage\": [],\n",
    "               \"entropy mean precision\": [], \"entropy mean coverage\": [],\n",
    "               \"GaussianMixtureModels mean precision\": [], \"GaussianMixtureModels mean coverage\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "accredited-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDataset():\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     datasets = data_dictionary[\"Data\"]\n",
    "    count = 0\n",
    "#     for data in datasets:\n",
    "        \n",
    "#         print(\"dataset number\", count)\n",
    "        \n",
    "#         results[\"Number\"].append(count)\n",
    "#         results[\"Name\"].append(data_dictionary[\"Name\"][count])\n",
    "\n",
    "\n",
    "#         train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(X_test, y_test, train_size=0.80)\n",
    "    blackbox_model = sklearn.ensemble.RandomForestClassifier(n_estimators=20)\n",
    "    blackbox_model.fit(train, labels_train)\n",
    "\n",
    "    explainer_1 = AnchorTabularExplainer(\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        train,\n",
    "        train_label = labels_train,\n",
    "        discretizer = 'quartile')\n",
    "\n",
    "    explainer_2 = AnchorTabularExplainer(\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        train,\n",
    "        train_label = labels_train,\n",
    "        discretizer = 'decile')\n",
    "\n",
    "    explainer_3 = AnchorTabularExplainer(\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        train,\n",
    "        train_label = labels_train,\n",
    "        discretizer = 'entropy')\n",
    "\n",
    "    explainer_4 = AnchorTabularExplainer(\n",
    "        target_names,\n",
    "        feature_names,\n",
    "        train,\n",
    "        train_label = labels_train,\n",
    "        discretizer = 'GaussianMixtureModels')\n",
    "\n",
    "\n",
    "    getMeanPrecisionCoverage(explainer_4, results, \"GaussianMixtureModels\")\n",
    "    getMeanPrecisionCoverage(explainer_1, results, \"quartile\")\n",
    "    getMeanPrecisionCoverage(explainer_2, results, \"decile\")\n",
    "    getMeanPrecisionCoverage(explainer_3, results, \"entropy\")\n",
    "\n",
    "\n",
    "    print(\"results\", results)\n",
    "    count += 1\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "immediate-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(X)\n",
    "labels_train = np.array(y)\n",
    "\n",
    "test1 = np.array(X_test)\n",
    "labels_test1 = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "arranged-peoples",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [], 'quartile mean coverage': [], 'decile mean precision': [], 'decile mean coverage': [], 'entropy mean precision': [], 'entropy mean coverage': [], 'GaussianMixtureModels mean precision': [0.964062686274484], 'GaussianMixtureModels mean coverage': [0.126633]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843], 'quartile mean coverage': [0.159416], 'decile mean precision': [], 'decile mean coverage': [], 'entropy mean precision': [], 'entropy mean coverage': [], 'GaussianMixtureModels mean precision': [0.964062686274484], 'GaussianMixtureModels mean coverage': [0.126633]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843], 'quartile mean coverage': [0.159416], 'decile mean precision': [0.9198142309863904], 'decile mean coverage': [0.138112], 'entropy mean precision': [], 'entropy mean coverage': [], 'GaussianMixtureModels mean precision': [0.964062686274484], 'GaussianMixtureModels mean coverage': [0.126633]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843], 'quartile mean coverage': [0.159416], 'decile mean precision': [0.9198142309863904], 'decile mean coverage': [0.138112], 'entropy mean precision': [0.9295078641705959], 'entropy mean coverage': [0.134798], 'GaussianMixtureModels mean precision': [0.964062686274484], 'GaussianMixtureModels mean coverage': [0.126633]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843], 'quartile mean coverage': [0.159416], 'decile mean precision': [0.9198142309863904], 'decile mean coverage': [0.138112], 'entropy mean precision': [0.9295078641705959], 'entropy mean coverage': [0.134798], 'GaussianMixtureModels mean precision': [0.964062686274484], 'GaussianMixtureModels mean coverage': [0.126633]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[:100]\n",
    "labels_test = labels_test1[:100]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-relaxation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ruled-style",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843], 'quartile mean coverage': [0.159416], 'decile mean precision': [0.9198142309863904], 'decile mean coverage': [0.138112], 'entropy mean precision': [0.9295078641705959], 'entropy mean coverage': [0.134798], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343], 'quartile mean coverage': [0.159416, 0.132466], 'decile mean precision': [0.9198142309863904], 'decile mean coverage': [0.138112], 'entropy mean precision': [0.9295078641705959], 'entropy mean coverage': [0.134798], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343], 'quartile mean coverage': [0.159416, 0.132466], 'decile mean precision': [0.9198142309863904, 0.935485698903857], 'decile mean coverage': [0.138112, 0.14564], 'entropy mean precision': [0.9295078641705959], 'entropy mean coverage': [0.134798], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343], 'quartile mean coverage': [0.159416, 0.132466], 'decile mean precision': [0.9198142309863904, 0.935485698903857], 'decile mean coverage': [0.138112, 0.14564], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467], 'entropy mean coverage': [0.134798, 0.158045], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343], 'quartile mean coverage': [0.159416, 0.132466], 'decile mean precision': [0.9198142309863904, 0.935485698903857], 'decile mean coverage': [0.138112, 0.14564], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467], 'entropy mean coverage': [0.134798, 0.158045], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[100:200]\n",
    "labels_test = labels_test1[100:200]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "narrow-plain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343], 'quartile mean coverage': [0.159416, 0.132466], 'decile mean precision': [0.9198142309863904, 0.935485698903857], 'decile mean coverage': [0.138112, 0.14564], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467], 'entropy mean coverage': [0.134798, 0.158045], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878], 'quartile mean coverage': [0.159416, 0.132466, 0.1528], 'decile mean precision': [0.9198142309863904, 0.935485698903857], 'decile mean coverage': [0.138112, 0.14564], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467], 'entropy mean coverage': [0.134798, 0.158045], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878], 'quartile mean coverage': [0.159416, 0.132466, 0.1528], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681], 'decile mean coverage': [0.138112, 0.14564, 0.137027], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467], 'entropy mean coverage': [0.134798, 0.158045], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878], 'quartile mean coverage': [0.159416, 0.132466, 0.1528], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681], 'decile mean coverage': [0.138112, 0.14564, 0.137027], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502], 'entropy mean coverage': [0.134798, 0.158045, 0.142472], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878], 'quartile mean coverage': [0.159416, 0.132466, 0.1528], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681], 'decile mean coverage': [0.138112, 0.14564, 0.137027], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502], 'entropy mean coverage': [0.134798, 0.158045, 0.142472], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[200:300]\n",
    "labels_test = labels_test1[200:300]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "simplified-complaint",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878], 'quartile mean coverage': [0.159416, 0.132466, 0.1528], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681], 'decile mean coverage': [0.138112, 0.14564, 0.137027], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502], 'entropy mean coverage': [0.134798, 0.158045, 0.142472], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681], 'decile mean coverage': [0.138112, 0.14564, 0.137027], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502], 'entropy mean coverage': [0.134798, 0.158045, 0.142472], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502], 'entropy mean coverage': [0.134798, 0.158045, 0.142472], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[300:400]\n",
    "labels_test = labels_test1[300:400]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "perfect-privacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[400:500]\n",
    "labels_test = labels_test1[400:500]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "periodic-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[500:600]\n",
    "labels_test = labels_test1[500:600]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "talented-philosophy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16281"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aggregate-marriage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202, 0.8848852313910525], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397, 0.1235025], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202, 0.8848852313910525], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397, 0.1235025], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339, 0.9257869493229853], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785, 0.1242235], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}\n",
      "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202, 0.8848852313910525], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397, 0.1235025], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339, 0.9257869493229853], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785, 0.1242235], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578, 0.93973720927555], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292, 0.1356115], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}\n",
      "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202, 0.8848852313910525], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397, 0.1235025], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339, 0.9257869493229853], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785, 0.1242235], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578, 0.93973720927555], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292, 0.1356115], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}\n"
     ]
    }
   ],
   "source": [
    "test = test1[600:800]\n",
    "labels_test = labels_test1[600:800]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "greek-interim",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (7) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (7). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (8). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (9). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n",
      "C:\\Users\\richa\\Anaconda3\\envs\\torch\\lib\\site-packages\\sklearn\\mixture\\_base.py:147: ConvergenceWarning: Number of distinct clusters (6) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
      "  label = cluster.KMeans(n_clusters=self.n_components, n_init=1,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-7345b1c6daff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabels_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_test1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresults_adult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-104-3677a5f94572>\u001b[0m in \u001b[0;36mtestDataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mgetMeanPrecisionCoverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplainer_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"GaussianMixtureModels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mgetMeanPrecisionCoverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplainer_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"quartile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgetMeanPrecisionCoverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplainer_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"decile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-102-31192484939d>\u001b[0m in \u001b[0;36mgetMeanPrecisionCoverage\u001b[1;34m(explainer, results, discretizer_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_coverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblackbox_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtest_precision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-86e85d1c42eb>\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, data_row, classifier_fn, threshold, delta, tau, batch_size, max_anchor_size, desired_label, beam_size, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m<=\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrigion\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                 labels = array of 1 and 0, 1 if the predictions is the same as the true labels, 0 otherwise \"\"\"\n\u001b[1;32m--> 448\u001b[1;33m         exp = AnchorBaseBeam.anchor_beam(\n\u001b[0m\u001b[0;32m    449\u001b[0m             \u001b[0msample_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[0mdesired_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_anchor_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_anchor_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-6695ad3ae517>\u001b[0m in \u001b[0;36manchor_beam\u001b[1;34m(sample_fn, delta, epsilon, batch_size, min_shared_samples, desired_confidence, beam_size, verbose, epsilon_stop, min_samples_start, max_anchor_size, verbose_every, stop_on_first, coverage_samples)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;34m\"\"\" still yet to undestand how this function works \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             chosen_tuples = AnchorBaseBeam.lucb(\n\u001b[0m\u001b[0;32m    397\u001b[0m                 \u001b[0msample_fns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m                 \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-6695ad3ae517>\u001b[0m in \u001b[0;36mlucb\u001b[1;34m(sample_fns, initial_stats, epsilon, delta, batch_size, top_n, verbose, verbose_every)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'B = %.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mn_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m             \u001b[0mpositives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msample_fns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpositives\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mut\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mn_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-6695ad3ae517>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(n, t)\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[0msample_fns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomplete_sample_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;31m#         print(\"sample_fns\", sample_fns)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-75-6695ad3ae517>\u001b[0m in \u001b[0;36mcomplete_sample_fn\u001b[1;34m(t, n)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0msample_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcomplete_sample_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mraw_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mcurrent_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'current_idx'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# idxs = range(state['data'].shape[0], state['data'].shape[0] + n)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-86e85d1c42eb>\u001b[0m in \u001b[0;36msample_fn\u001b[1;34m(present, num_samples, compute_labels)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     \u001b[0mRandomelly\u001b[0m \u001b[0mselects\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mreplacement\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msaved\u001b[0m \u001b[0mreagio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     and samples as many samples from the boostrap <=  to the saved reagion, and vice versa\"\"\"\n\u001b[1;32m--> 385\u001b[1;33m             raw_data = self.sample_from_train(\n\u001b[0m\u001b[0;32m    386\u001b[0m                 conditions_eq, {}, conditions_geq, conditions_leq, num_samples)\n\u001b[0;32m    387\u001b[0m             \u001b[0md_raw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-86e85d1c42eb>\u001b[0m in \u001b[0;36msample_from_train\u001b[1;34m(self, conditions_eq, conditions_neq, conditions_geq, conditions_leq, num_samples)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconditions_leq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mconditions_leq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;31m#             print('self.min[f]', self.min[f])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = test1[800:900]\n",
    "labels_test = labels_test1[800:900]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test1[900:1000]\n",
    "labels_test = labels_test1[900:1000]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test1[1000:1500]\n",
    "labels_test = labels_test1[1000:1500]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test1[1500:2000]\n",
    "labels_test = labels_test1[1500:2000]\n",
    "results_adult = testDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-aquatic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "equipped-delivery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['GaussianMixtureModels mean coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interesting-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927, 0.9034954121072202, 0.8848852313910525], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117, 0.120397, 0.1235025], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442, 0.9421492560461339, 0.9257869493229853], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434, 0.123785, 0.1242235], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045, 0.9427826159036578, 0.93973720927555], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444, 0.124292, 0.1356115], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956, 0.9661992548085844, 0.9671578261725734], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235, 0.122005, 0.131659]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regulated-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.887214373354864, 0.1383862142857143)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "statistics.mean(results['quartile mean precision']), statistics.mean(results['quartile mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunset-invention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9262716502013812, 0.1366167857142857)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(results['decile mean precision']),  statistics.mean(results['decile mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifteen-clinton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9384381329312185, 0.1402317857142857)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(results['entropy mean precision']),  statistics.mean(results['entropy mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "found-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9660254879352757, 0.1334152857142857)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(results['GaussianMixtureModels mean precision']),  statistics.mean(results['GaussianMixtureModels mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile entropy GaussianMixtureModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(results_tests['quartile mean precision']), statistics.mean(results_tests['quartile mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(results_tests['decile mean precision']), statistics.mean(results_tests['decile mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(results_tests['entropy mean precision']), statistics.mean(results_tests['entropy mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(results_tests['GaussianMixtureModels mean precision']), statistics.mean(results_tests['GaussianMixtureModels mean coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-assist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-resistance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-soldier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-event",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-jason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[1000:1500]\n",
    "labels_test = labels_test[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-apollo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}\n",
    "results {'Number': [], 'Name': [], 'quartile mean precision': [0.8734933547041843, 0.9021624157790343, 0.8845284480733878, 0.8715482940341757, 0.8903874573949927], 'quartile mean coverage': [0.159416, 0.132466, 0.1528, 0.124005, 0.156117], 'decile mean precision': [0.9198142309863904, 0.935485698903857, 0.9202325780066681, 0.91643348480619, 0.9239993533374442], 'decile mean coverage': [0.138112, 0.14564, 0.137027, 0.128096, 0.159434], 'entropy mean precision': [0.9295078641705959, 0.9407129830299467, 0.9411758916810502, 0.9363317535821238, 0.9388186128756045], 'entropy mean coverage': [0.134798, 0.158045, 0.142472, 0.13396, 0.152444], 'GaussianMixtureModels mean precision': [0.964062686274484, 0.966787906868761, 0.9649019625507983, 0.9687371508670325, 0.9643316280046956], 'GaussianMixtureModels mean coverage': [0.126633, 0.143927, 0.137842, 0.120606, 0.151235]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-somewhere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-kennedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-banking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 4 <= 1.00 AND 12 <= 3.00 AND 1 <= 2.00 AND 9 <= 1.00 AND 3 <= 1.00 AND 13 <= 0.00 AND 0 <= 11.00\n",
    "# Precision: 0.91\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 1.00 < 4 <= 2.00 AND 10 <= 1.00 AND 8 > 0.00 AND 11 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 13 <= 0.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 9 <= 0.00 AND 8 <= 0.00 AND 6 <= 2.00 AND 0 <= 11.00 AND 12 <= 9.00 AND 2 <= 3940.00 AND 3 <= 5.00 AND 4 <= 5.00\n",
    "# Precision: 0.46\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.15\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 8 > 0.00 AND 3 > 2.00 AND 11 <= 0.00 AND 12 <= 3.00 AND 1 > 2.00 AND 4 <= 5.00 AND 0 <= 22.00 AND 13 <= 0.00\n",
    "# Precision: 0.91\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# quartile\n",
    "# Anchor: 7 <= 3.00 AND 0.00 < 5 <= 1.00 AND 6 <= 7.00\n",
    "# Precision: 0.48\n",
    "# Coverage: 0.37\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 6 <= 2.00 AND 3 <= 1.00 AND 9 <= 0.00 AND 12 <= 9.00 AND 13 <= 0.00 AND 0 <= 11.00 AND 4 <= 1.00 AND 1 <= 2.00\n",
    "# Precision: 0.78\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 4 > 5.00 AND 10 <= 1.00 AND 8 > 0.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 11.00 < 0 <= 22.00 AND 3 <= 2.00 AND 6 > 2.00 AND 9 <= 1.00 AND 2 <= 14408.00 AND 4 <= 2.00 AND 12 <= 9.00 AND 13 <= 0.00\n",
    "# Precision: 0.93\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 9 <= 0.00 AND 0.00 < 7 <= 1.00 AND 6 <= 2.00 AND 4 <= 1.00 AND 2 <= 3940.00 AND 0 <= 22.00 AND 13 <= 0.00 AND 11 <= 0.00 AND 3 <= 1.00 AND 12 <= 9.00\n",
    "# Precision: 0.78\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 13 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.31\n",
    "# quartile\n",
    "# Anchor: 7 > 3.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.09\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.08\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 8 > 0.00 AND 10 <= 1.00 AND 4 > 5.00 AND 0 > 11.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 9 <= 0.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 13 <= 0.00 AND 0 <= 34.00\n",
    "# Precision: 0.48\n",
    "# Coverage: 0.27\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 0.00 < 5 <= 1.00 AND 9 <= 0.00 AND 3 > 2.00 AND 7 <= 1.00 AND 4 > 2.00\n",
    "# Precision: 0.46\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 8 > 0.00 AND 11 <= 0.00 AND 12 > 9.00 AND 0 > 11.00\n",
    "# Precision: 0.90\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 10 <= 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 12 > 3.00 AND 1 <= 2.00 AND 0 > 11.00\n",
    "# Precision: 0.93\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 9 <= 0.00 AND 0.00 < 5 <= 1.00 AND 13 <= 0.00 AND 0.00 < 7 <= 1.00\n",
    "# Precision: 0.44\n",
    "# Coverage: 0.37\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 9 > 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.08\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 3 > 2.00 AND 8 > 0.00 AND 10 <= 1.00 AND 4 <= 5.00 AND 1 <= 2.00 AND 11 <= 0.00 AND 12 <= 0.00 AND 9 <= 1.00 AND 6 <= 2.00 AND 0 <= 22.00 AND 2 <= 14408.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 6 > 7.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.07\n",
    "# quartile\n",
    "# Anchor: 8 > 0.00 AND 12 <= 0.00 AND 10 <= 1.00 AND 3 <= 2.00 AND 6 > 2.00 AND 1 <= 2.00 AND 11 <= 0.00 AND 13 <= 0.00 AND 4 <= 2.00 AND 2 <= 14408.00 AND 9 <= 1.00\n",
    "# Precision: 0.93\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 6 <= 7.00 AND 2 <= 14408.00\n",
    "# Precision: 0.50\n",
    "# Coverage: 0.26\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.31\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 8 > 0.00 AND 10 <= 1.00 AND 4 > 5.00 AND 11 <= 0.00 AND 1 <= 2.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 4 > 5.00 AND 12 > 9.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 4 <= 2.00 AND 8 > 0.00 AND 10 <= 1.00 AND 12 <= 0.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 9 <= 1.00 AND 0 <= 11.00 AND 3 <= 5.00 AND 13 <= 0.00 AND 2 <= 8661.00 AND 6 <= 2.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 8 > 0.00 AND 10 <= 1.00 AND 4 <= 2.00 AND 6 > 2.00 AND 12 <= 0.00 AND 11 <= 0.00 AND 2 <= 14408.00 AND 1 <= 2.00 AND 9 <= 1.00 AND 13 <= 0.00 AND 3 <= 5.00\n",
    "# Precision: 0.94\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 7 > 3.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 11 <= 0.00 AND 0.00 < 5 <= 1.00 AND 6 <= 7.00 AND 9 <= 1.00 AND 0 <= 34.00\n",
    "# Precision: 0.46\n",
    "# Coverage: 0.25\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 2.00 < 3 <= 5.00 AND 9 <= 0.00 AND 2 <= 3940.00 AND 4 <= 5.00 AND 13 <= 0.00 AND 1 <= 2.00 AND 0.00 < 12 <= 3.00 AND 22.00 < 0 <= 34.00 AND 11 <= 0.00 AND 6 > 7.00 AND 8 > 0.00\n",
    "# Precision: 0.74\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 4 <= 5.00 AND 5 > 0.00 AND 2 <= 3940.00 AND 0.00 < 7 <= 3.00 AND 8 <= 0.00\n",
    "# Precision: 0.39\n",
    "# Coverage: 0.09\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 6 > 7.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.02\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 5 > 0.00 AND 7 <= 1.00 AND 9 <= 0.00 AND 8 <= 0.00 AND 13 <= 0.00 AND 2 <= 8661.00\n",
    "# Precision: 0.40\n",
    "# Coverage: 0.21\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.30\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 6 > 7.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.07\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.30\n",
    "# quartile\n",
    "# Anchor: 9 <= 1.00 AND 0.00 < 5 <= 1.00\n",
    "# Precision: 0.45\n",
    "# Coverage: 0.46\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 13 <= 0.00 AND 9 <= 0.00 AND 0 <= 11.00 AND 0.00 < 12 <= 3.00 AND 1 <= 2.00 AND 11 <= 0.00 AND 8 > 0.00 AND 2 <= 8661.00 AND 3 > 2.00\n",
    "# Precision: 0.70\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 3 <= 1.00 AND 0.00 < 7 <= 1.00 AND 9 <= 0.00 AND 4 <= 1.00 AND 2 <= 3940.00 AND 13 <= 0.00 AND 22.00 < 0 <= 34.00 AND 0.00 < 12 <= 3.00 AND 8 > 0.00 AND 1 > 2.00 AND 11 <= 0.00 AND 6 > 4.00\n",
    "# Precision: 0.73\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 0 > 22.00 AND 9 <= 1.00 AND 4 <= 2.00 AND 12 <= 3.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# quartile\n",
    "# Anchor: 0 <= 34.00 AND 5 > 0.00 AND 9 <= 0.00 AND 3 > 2.00 AND 7 > 0.00 AND 13 <= 0.00\n",
    "# Precision: 0.48\n",
    "# Coverage: 0.12\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 4.00 < 6 <= 7.00 AND 9 <= 0.00 AND 4 <= 2.00 AND 13 <= 0.00 AND 1 > 2.00 AND 3 <= 2.00 AND 2 <= 3940.00\n",
    "# Precision: 0.76\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.11\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 0 > 11.00 AND 12 > 3.00\n",
    "# Precision: 0.94\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 9 <= 0.00 AND 2.00 < 4 <= 5.00 AND 3 <= 5.00 AND 0 <= 11.00 AND 2 <= 8661.00 AND 8 > 0.00 AND 12 > 3.00 AND 13 <= 0.00\n",
    "# Precision: 0.72\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 4 > 5.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 13 > 0.00 AND 10 <= 1.00 AND 8 > 0.00 AND 4 <= 1.00 AND 12 <= 0.00 AND 1 <= 2.00 AND 11 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 6 > 7.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.07\n",
    "# quartile\n",
    "# Anchor: 6 > 4.00 AND 13 > 0.00 AND 10 <= 1.00 AND 3 > 1.00 AND 11 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# quartile\n",
    "# Anchor: 0 > 34.00 AND 8 > 0.00 AND 10 <= 1.00 AND 1 <= 2.00 AND 11 <= 0.00 AND 4 <= 2.00 AND 13 <= 0.00 AND 2 <= 14408.00 AND 12 <= 3.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0 > 34.00 AND 11 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0 > 34.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 12 <= 3.00 AND 4 <= 5.00 AND 13 <= 0.00 AND 11.00 < 0 <= 22.00 AND 1.00 < 3 <= 5.00 AND 9 <= 1.00 AND 2 <= 8661.00\n",
    "# Precision: 0.91\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 7 <= 1.00 AND 0.00 < 5 <= 1.00 AND 6 <= 7.00 AND 3 <= 5.00 AND 0 <= 11.00 AND 9 <= 0.00 AND 0.00 < 12 <= 3.00 AND 4 <= 1.00 AND 1 > 2.00 AND 2 <= 8661.00\n",
    "# Precision: 0.78\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 9 <= 0.00 AND 6 <= 7.00 AND 13 <= 0.00 AND 12 <= 9.00 AND 2 <= 14408.00 AND 4 <= 5.00 AND 3 <= 5.00 AND 1 > 2.00\n",
    "# Precision: 0.52\n",
    "# Coverage: 0.03\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 12 > 9.00 AND 3 <= 2.00 AND 1 <= 2.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 7 > 3.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.09\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.33\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 6 <= 7.00 AND 7 > 0.00 AND 13 <= 0.00\n",
    "# Precision: 0.67\n",
    "# Coverage: 0.04\n",
    "# quartile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.10\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 6 > 7.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.07\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.15\n",
    "# quartile\n",
    "# Anchor: 0 > 34.00 AND 8 > 0.00 AND 10 <= 1.00 AND 12 <= 0.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 3 <= 5.00 AND 13 <= 0.00 AND 4 <= 5.00 AND 9 <= 1.00\n",
    "# Precision: 0.92\n",
    "# Coverage: 0.01\n",
    "# quartile\n",
    "# Anchor: 6 > 7.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0 > 34.00 AND 11 <= 0.00 AND 1 <= 2.00 AND 2 <= 14408.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# quartile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 <= 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.05\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0.00 < 3 <= 2.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 8 > 0.00 AND 10 <= 1.00 AND 3 > 0.00 AND 4 <= 2.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 9 <= 1.00 AND 0.00 < 5 <= 1.00 AND 6 <= 3.00\n",
    "# Precision: 0.59\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.31\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 2.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# decile\n",
    "# Anchor: 3.00 < 6 <= 5.00 AND 8 > 0.00 AND 4 > 0.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 2.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 0.00 < 8 <= 1.00 AND 0.00 < 5 <= 1.00 AND 5.00 < 6 <= 6.00 AND 4.00 < 0 <= 9.00 AND 9 <= 0.00 AND 0.00 < 7 <= 1.00 AND 13 <= 0.00 AND 3.00 < 12 <= 4.00 AND 2 <= 1494.00 AND 2.00 < 4 <= 8.00 AND 2.00 < 3 <= 8.00 AND 11 <= 0.00 AND 1 <= 1.00 AND 10 <= 1.00\n",
    "# Precision: 0.63\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 6 <= 1.00 AND 0.00 < 7 <= 1.00 AND 0 <= 13.00 AND 8 <= 1.00 AND 13 <= 0.00 AND 12 <= 0.00 AND 3 <= 1.00 AND 9 <= 0.00 AND 4 <= 1.00 AND 1 <= 2.00 AND 2 <= 1494.00 AND 11 <= 0.00\n",
    "# Precision: 0.80\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 10 <= 1.00 AND 3 > 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.04\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 0.00 < 4 <= 1.00 AND 8 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.05\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 6 <= 3.00 AND 0.00 < 7 <= 1.00 AND 12 <= 23.00 AND 0 <= 22.00 AND 9 <= 0.00 AND 8 <= 1.00 AND 2 <= 15731.00 AND 13 <= 0.00 AND 1 <= 2.00\n",
    "# Precision: 0.79\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 0.00 < 8 <= 1.00 AND 10 <= 1.00 AND 0 <= 27.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 7 > 3.00 AND 6 > 6.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.03\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 9 > 0.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.16\n",
    "# decile\n",
    "# Anchor: 3 > 8.00 AND 6 > 3.00 AND 10 <= 1.00 AND 8 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 0.00 < 8 <= 1.00 AND 0.00 < 5 <= 1.00 AND 4.00 < 0 <= 9.00 AND 0.00 < 7 <= 1.00 AND 8.00 < 12 <= 12.00 AND 8.00 < 6 <= 10.00 AND 9 <= 0.00 AND 5.00 < 4 <= 8.00 AND 13 <= 0.00 AND 2 <= 1494.00 AND 5.00 < 3 <= 8.00 AND 1 <= 1.00 AND 11 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.65\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.07\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 0.00 < 7 <= 1.00 AND 0.00 < 5 <= 1.00 AND 13 <= 0.00 AND 4.00 < 6 <= 5.00 AND 0.00 < 8 <= 1.00 AND 9 <= 0.00 AND 12 <= 0.00 AND 0 <= 4.00 AND 2 <= 1494.00 AND 5.00 < 4 <= 8.00 AND 5.00 < 3 <= 8.00 AND 11 <= 0.00 AND 1 <= 1.00 AND 10 <= 1.00\n",
    "# Precision: 0.64\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 6 > 5.00 AND 8 > 0.00 AND 0.00 < 3 <= 2.00 AND 11 <= 0.00 AND 0 <= 32.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 > 5.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.10\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 10 <= 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.28\n",
    "# decile\n",
    "# Anchor: 6 > 5.00 AND 8 > 0.00 AND 10 <= 1.00 AND 12 > 4.00 AND 0.00 < 4 <= 2.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 <= 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 0.00 < 7 <= 2.00 AND 0.00 < 5 <= 1.00 AND 12 > 0.00 AND 0 <= 37.00 AND 6 <= 8.00\n",
    "# Precision: 0.55\n",
    "# Coverage: 0.17\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 9 > 0.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.17\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 <= 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0.00 < 3 <= 5.00 AND 9.00 < 0 <= 27.00 AND 11 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 6 > 6.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.07\n",
    "# decile\n",
    "# Anchor: 0.00 < 4 <= 2.00 AND 8 > 0.00 AND 13 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.04\n",
    "# decile\n",
    "# Anchor: 0.00 < 7 <= 2.00 AND 0.00 < 5 <= 1.00 AND 6 <= 5.00 AND 0 <= 22.00 AND 9 <= 1.00\n",
    "# Precision: 0.58\n",
    "# Coverage: 0.14\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 > 5.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.10\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 6 > 3.00 AND 10 <= 1.00 AND 8 > 0.00 AND 3 > 5.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 6 > 8.00 AND 0.00 < 8 <= 1.00 AND 4 > 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0.00 < 4 <= 2.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 3.00 < 6 <= 4.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 6 > 3.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 7 <= 4.00 AND 0.00 < 5 <= 1.00 AND 6 <= 6.00 AND 8 <= 1.00 AND 12 <= 23.00\n",
    "# Precision: 0.52\n",
    "# Coverage: 0.30\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 4 <= 5.00 AND 0.00 < 12 <= 4.00 AND 13 <= 0.00 AND 9 <= 0.00 AND 3 <= 5.00 AND 2 <= 3065.00 AND 22.00 < 0 <= 48.00 AND 1 <= 2.00 AND 8 <= 1.00\n",
    "# Precision: 0.75\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 13 <= 2.00 AND 3 <= 0.00 AND 5 > 0.00 AND 4 <= 0.00 AND 1.00 < 7 <= 2.00 AND 2 <= 1494.00 AND 8 <= 0.00 AND 6 > 10.00 AND 27.00 < 0 <= 37.00 AND 12 <= 23.00 AND 9 > 0.00 AND 1 <= 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.10\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 8 > 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.06\n",
    "# decile\n",
    "# Anchor: 12 <= 23.00 AND 5 > 0.00 AND 9 <= 0.00 AND 2 <= 3065.00\n",
    "# Precision: 0.43\n",
    "# Coverage: 0.10\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.31\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 <= 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 6 > 6.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.08\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 > 6.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.08\n",
    "# decile\n",
    "# Anchor: 8 <= 1.00 AND 0.00 < 5 <= 1.00 AND 0 <= 37.00 AND 13 <= 0.00 AND 0.00 < 7 <= 3.00\n",
    "# Precision: 0.48\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 7 <= 1.00 AND 0.00 < 6 <= 8.00 AND 9 <= 0.00 AND 13 <= 0.00 AND 0 <= 9.00 AND 4 <= 8.00 AND 0.00 < 12 <= 8.00 AND 8 <= 1.00 AND 3 <= 8.00 AND 2 <= 15731.00\n",
    "# Precision: 0.74\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 3 <= 1.00 AND 6 <= 8.00 AND 0.00 < 7 <= 1.00 AND 1 > 4.00 AND 4 <= 1.00 AND 9 <= 0.00 AND 2 <= 6674.00 AND 0.00 < 12 <= 23.00 AND 8 <= 1.00 AND 13 <= 0.00 AND 0 <= 32.00\n",
    "# Precision: 0.79\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 10 <= 1.00 AND 0.00 < 3 <= 2.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# decile\n",
    "# Anchor: 13 <= 0.00 AND 5 > 0.00 AND 9 <= 0.00 AND 0.00 < 7 <= 2.00 AND 3 > 2.00 AND 6 <= 10.00 AND 0 <= 37.00\n",
    "# Precision: 0.49\n",
    "# Coverage: 0.12\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 <= 0.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.05\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 6 > 6.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 4.00 < 6 <= 6.00 AND 0.00 < 7 <= 1.00 AND 13 <= 0.00 AND 32.00 < 0 <= 37.00 AND 8 <= 1.00 AND 12.00 < 12 <= 23.00 AND 9 <= 0.00 AND 1 > 4.00 AND 1.00 < 4 <= 5.00 AND 3 <= 2.00 AND 2 <= 6674.00 AND 11 <= 0.00\n",
    "# Precision: 0.78\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 6 > 3.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 2.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.04\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 10 <= 1.00 AND 3 > 0.00 AND 4 <= 2.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 6 <= 8.00 AND 13 <= 0.00 AND 0.00 < 7 <= 1.00 AND 8 <= 1.00 AND 12 <= 12.00 AND 4 <= 5.00 AND 3 <= 5.00 AND 9 <= 0.00 AND 2 <= 6674.00 AND 4.00 < 0 <= 37.00\n",
    "# Precision: 0.74\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 10 <= 1.00 AND 3 > 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.04\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 6 <= 0.00 AND 13 > 0.00 AND 8 > 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 6 > 6.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.08\n",
    "# decile\n",
    "# Anchor: 6 > 5.00 AND 8 > 0.00 AND 10 <= 1.00 AND 4 > 0.00 AND 13 > 2.00 AND 11 <= 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 6 <= 0.00 AND 8 > 0.00 AND 0.00 < 4 <= 1.00 AND 10 <= 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 3 > 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 10 <= 1.00 AND 3 > 0.00 AND 4 <= 2.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.02\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 6 <= 0.00 AND 8 > 0.00 AND 3 > 0.00 AND 10 <= 1.00 AND 11 <= 0.00 AND 17.00 < 0 <= 27.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.17\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.10\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 4.00 < 6 <= 6.00 AND 9 <= 0.00 AND 0.00 < 7 <= 1.00 AND 13 <= 2.00 AND 0.00 < 12 <= 8.00 AND 8 <= 1.00 AND 0 <= 32.00 AND 3 <= 1.00 AND 4 <= 1.00 AND 1 > 4.00 AND 2 <= 3065.00\n",
    "# Precision: 0.79\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 0.00 < 7 <= 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 6 <= 6.00 AND 0.00 < 12 <= 4.00 AND 9 <= 0.00 AND 0 <= 48.00 AND 2 <= 1494.00 AND 13 <= 0.00 AND 8 <= 1.00 AND 2.00 < 1 <= 4.00 AND 2.00 < 4 <= 5.00 AND 3 <= 5.00\n",
    "# Precision: 0.50\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 > 1.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.07\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.31\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 8 > 0.00 AND 3 > 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.04\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# decile\n",
    "# Anchor: 7 > 2.00 AND 9 > 0.00\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.16\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.31\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# decile\n",
    "# Anchor: 10 > 1.00 AND 0.00 < 5 <= 1.00 AND 0.00 < 6 <= 5.00 AND 3.00 < 7 <= 4.00 AND 13 <= 0.00 AND 8 <= 1.00 AND 2 <= 3065.00 AND 9 <= 1.00 AND 3.00 < 12 <= 4.00 AND 5.00 < 3 <= 8.00 AND 5.00 < 4 <= 8.00 AND 1 <= 1.00 AND 0 <= 32.00 AND 11 <= 0.00\n",
    "# Precision: 0.72\n",
    "# Coverage: 0.00\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 6 > 6.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.08\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 7 > 1.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# decile\n",
    "# Anchor: 5 <= 0.00 AND 9 > 0.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.14\n",
    "# decile\n",
    "# Anchor: 1 <= 1.00 AND 0.00 < 8 <= 1.00 AND 10 <= 1.00 AND 4 > 0.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 6 > 6.00 AND 10 <= 1.00 AND 0 > 37.00 AND 8 > 0.00 AND 11 <= 0.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.01\n",
    "# decile\n",
    "# Anchor: 7 > 1.00 AND 5 <= 0.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 6 <= 0.50\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.05\n",
    "# entropy\n",
    "# Anchor: 8 > 0.50 AND 10 <= 0.50 AND 0.50 < 3 <= 2.50 AND 4 > 0.50 AND 6 <= 4.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 8 > 0.50 AND 4 > 0.50 AND 1.50 < 3 <= 2.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# entropy\n",
    "# Anchor: 0.50 < 10 <= 55.50 AND 0.50 < 7 <= 2.50 AND 0.50 < 5 <= 1.50 AND 6 <= 1.50 AND 8 <= 0.50 AND 0.50 < 0 <= 3.50 AND 2.50 < 3 <= 3.50 AND 2.50 < 4 <= 3.50 AND 2 <= 1775.50 AND 13 <= 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 5 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# entropy\n",
    "# Anchor: 6 > 3.50 AND 0.50 < 8 <= 1.50 AND 10 <= 5.50 AND 11 <= 2.50 AND 3 > 0.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.06\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 2.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 13 <= 4.50 AND 0.50 < 5 <= 1.50 AND 6 <= 6.50 AND 12 > 3.50 AND 7 <= 2.50 AND 8 <= 2.50 AND 2 <= 21513.50\n",
    "# Precision: 0.55\n",
    "# Coverage: 0.14\n",
    "# entropy\n",
    "# Anchor: 1.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 6 <= 1.50 AND 0 <= 11.50 AND 3 <= 3.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 3 > 0.50 AND 11 <= 0.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 8 > 0.50 AND 10 <= 0.50 AND 0.50 < 4 <= 2.50 AND 1 <= 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 1.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 2.50 AND 6 <= 1.50 AND 2 <= 833.50 AND 1 <= 4.50 AND 13 <= 2.50 AND 4 <= 2.50 AND 3 <= 3.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 3 > 3.50 AND 11 <= 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 10 <= 1.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.27\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 9 > 0.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.17\n",
    "# entropy\n",
    "# Anchor: 6 > 3.50 AND 8 > 0.50 AND 10 <= 1.50 AND 4 > 0.50 AND 12 > 5.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 0 > 0.50 AND 0.50 < 5 <= 1.50 AND 12 > 2.50 AND 9 <= 0.50 AND 7 <= 1.50 AND 13 <= 1.50 AND 2 <= 21513.50 AND 1 <= 6.50 AND 8 <= 1.50 AND 10 <= 55.50 AND 6 > 0.50\n",
    "# Precision: 0.52\n",
    "# Coverage: 0.17\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 10 <= 1.50 AND 5 > 1.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.09\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# entropy\n",
    "# Anchor: 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 4.50 < 6 <= 6.50 AND 9 <= 0.50 AND 13 <= 1.50 AND 1 <= 2.50 AND 11 <= 0.50 AND 2 <= 833.50 AND 8 <= 1.50 AND 10 <= 0.50 AND 12 <= 5.50 AND 0 <= 0.50 AND 3 > 0.50\n",
    "# Precision: 0.51\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 6 > 3.50 AND 8 > 0.50 AND 10 <= 5.50 AND 4 > 0.50 AND 3 <= 2.50 AND 11 <= 0.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.04\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 10 <= 1.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.28\n",
    "# entropy\n",
    "# Anchor: 6 > 3.50 AND 8 > 0.50 AND 0.50 < 4 <= 2.50 AND 0 > 9.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.19\n",
    "# entropy\n",
    "# Anchor: 4.50 < 1 <= 5.50 AND 0.50 < 5 <= 1.50 AND 12 > 3.50 AND 3 <= 3.50 AND 0.50 < 7 <= 1.50 AND 2.50 < 4 <= 3.50 AND 13 <= 1.50 AND 2 <= 833.50 AND 0.50 < 10 <= 12.50 AND 8 <= 0.50 AND 11 <= 12.50 AND 6 > 4.50 AND 9 <= 0.50 AND 11.50 < 0 <= 27.50\n",
    "# Precision: 0.83\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 9 > 0.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.16\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 8 > 0.50 AND 3 > 0.50 AND 10 <= 1.50 AND 12 <= 1.50 AND 0.50 < 4 <= 3.50 AND 1 <= 2.50 AND 11 <= 2.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 7 > 2.50 AND 6 > 6.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.08\n",
    "# entropy\n",
    "# Anchor: 8 > 0.50 AND 3 > 0.50 AND 4 <= 2.50 AND 10 <= 1.50 AND 12 <= 1.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 13 <= 5.50 AND 0.50 < 5 <= 1.50 AND 6 <= 6.50 AND 7 <= 2.50 AND 0 <= 11.50\n",
    "# Precision: 0.51\n",
    "# Coverage: 0.10\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.33\n",
    "# entropy\n",
    "# Anchor: 6 > 3.50 AND 8 > 0.50 AND 10 <= 1.50 AND 4 > 0.50 AND 9.50 < 0 <= 27.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 4 > 0.50 AND 11 <= 2.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.04\n",
    "# entropy\n",
    "# Anchor: 0.50 < 4 <= 2.50 AND 8 > 0.50 AND 10 <= 1.50 AND 11 <= 2.50 AND 12 <= 2.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 4 <= 1.50 AND 8 > 0.50 AND 3.50 < 6 <= 4.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.01\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# entropy\n",
    "# Anchor: 7 > 3.50 AND 9 > 0.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.10\n",
    "# entropy\n",
    "# Anchor: 0.50 < 5 <= 1.50 AND 0.50 < 6 <= 6.50 AND 9 <= 0.50 AND 4 > 2.50 AND 13 <= 4.50 AND 3 > 2.50 AND 7 <= 2.50 AND 10 <= 55.50\n",
    "# Precision: 0.57\n",
    "# Coverage: 0.11\n",
    "# entropy\n",
    "# Anchor: 1.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 4 <= 3.50 AND 0.50 < 12 <= 5.50 AND 13 <= 1.50 AND 0 > 10.50 AND 6 > 6.50 AND 9 <= 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 8 <= 0.50 AND 4 <= 0.50 AND 1.50 < 7 <= 2.50 AND 5 > 0.50 AND 3 <= 0.50 AND 2 <= 833.50 AND 13 <= 1.50 AND 6 > 6.50 AND 0 > 11.50 AND 1 <= 2.50 AND 10 <= 0.50 AND 11 <= 0.50 AND 9 > 0.50\n",
    "# Precision: 0.76\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 6 > 6.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.08\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.32\n",
    "# entropy\n",
    "# Anchor: 10 <= 55.50 AND 5 > 0.50 AND 9 <= 0.50 AND 7 <= 1.50 AND 3 <= 3.50 AND 13 <= 1.50 AND 6 > 0.50 AND 8 <= 1.50 AND 11 <= 12.50 AND 2 <= 21513.50 AND 4 <= 3.50 AND 1 <= 6.50\n",
    "# Precision: 0.44\n",
    "# Coverage: 0.23\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 5 > 2.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.04\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 6 > 6.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.08\n",
    "# entropy\n",
    "# Anchor: 11 <= 5.50 AND 0.50 < 5 <= 1.50 AND 6 <= 6.50 AND 2 <= 11003.50 AND 13 <= 4.50 AND 9 <= 0.50 AND 8 <= 2.50\n",
    "# Precision: 0.55\n",
    "# Coverage: 0.17\n",
    "# entropy\n",
    "# Anchor: 1.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 13 <= 1.50 AND 1 <= 4.50 AND 9 <= 0.50 AND 0.50 < 8 <= 1.50 AND 0.50 < 12 <= 5.50 AND 4.50 < 0 <= 10.50 AND 2 <= 21513.50 AND 11 <= 16.50\n",
    "# Precision: 0.92\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5.50 < 10 <= 12.50 AND 1 > 5.50 AND 0.50 < 5 <= 1.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 4 > 0.50 AND 3 <= 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.19\n",
    "# entropy\n",
    "# Anchor: 0.50 < 7 <= 2.50 AND 5 > 0.50 AND 9 <= 0.50 AND 2 <= 833.50 AND 13 <= 1.50 AND 10 <= 0.50 AND 2.50 < 3 <= 3.50 AND 0.50 < 8 <= 1.50 AND 2.50 < 4 <= 3.50 AND 6 > 4.50 AND 1 <= 2.50 AND 12 <= 3.50 AND 11 <= 16.50\n",
    "# Precision: 0.73\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 9 > 0.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.14\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 6 > 6.50 AND 5 > 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5.50 < 10 <= 12.50 AND 1 > 5.50 AND 0.50 < 5 <= 1.50 AND 7 <= 2.50 AND 4 <= 3.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 6 > 6.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.08\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 3 > 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 5.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 3 <= 3.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 0.50 < 8 <= 1.50 AND 10 <= 1.50 AND 3 > 0.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 13 > 2.50 AND 6 <= 0.50 AND 10 <= 1.50 AND 0.50 < 3 <= 2.50 AND 8 > 0.50\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 5 > 1.50 AND 6 > 6.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 8 > 2.50 AND 6 > 3.50 AND 10 <= 1.50 AND 13 > 4.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.33\n",
    "# entropy\n",
    "# Anchor: 6 <= 0.50 AND 8 > 0.50 AND 10 <= 1.50 AND 3 > 0.50 AND 4 <= 2.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.01\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 10 <= 0.50 AND 4 > 0.50 AND 0 > 9.50 AND 11 <= 2.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 10 <= 1.50 AND 0.50 < 8 <= 1.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 6 > 6.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.08\n",
    "# entropy\n",
    "# Anchor: 6 <= 0.50 AND 8 > 0.50 AND 0.50 < 3 <= 3.50 AND 10 <= 1.50 AND 11 <= 2.50 AND 0 <= 27.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 5 > 2.50 AND 6 > 3.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.03\n",
    "# entropy\n",
    "# Anchor: 5.50 < 10 <= 12.50 AND 1 > 5.50 AND 0.50 < 5 <= 1.50 AND 7 <= 2.50 AND 3 <= 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 1.50 AND 4.50 < 6 <= 6.50 AND 13 <= 1.50 AND 0 > 11.50 AND 10 <= 0.50 AND 2.50 < 1 <= 4.50 AND 11 <= 0.50 AND 2 <= 833.50 AND 9 <= 0.50 AND 0.50 < 8 <= 1.50 AND 0.50 < 12 <= 2.50 AND 2.50 < 4 <= 3.50 AND 2.50 < 3 <= 3.50\n",
    "# Precision: 0.81\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 8 > 0.50 AND 10 <= 1.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.07\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 1.50\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.31\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 3 > 0.50 AND 4 <= 2.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.00\n",
    "# entropy\n",
    "# Anchor: 7 > 3.50 AND 9 > 0.50\n",
    "# Precision: 0.97\n",
    "# Coverage: 0.10\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.32\n",
    "# entropy\n",
    "# Anchor: 5.50 < 10 <= 12.50 AND 0.50 < 5 <= 1.50 AND 0.50 < 7 <= 4.50 AND 13 <= 1.50 AND 8 <= 1.50 AND 11 <= 5.50 AND 0 > 0.50 AND 12 <= 5.50 AND 6 > 0.50 AND 2 <= 11013.50 AND 1 <= 6.50\n",
    "# Precision: 0.91\n",
    "# Coverage: 0.01\n",
    "# entropy\n",
    "# Anchor: 7 > 1.50 AND 5 > 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.04\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 7 > 1.50\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.17\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.33\n",
    "# entropy\n",
    "# Anchor: 3 > 3.50 AND 0.50 < 8 <= 1.50 AND 10 <= 1.50 AND 0 > 9.50 AND 4 > 0.50 AND 11 <= 2.50 AND 1 <= 2.50\n",
    "# Precision: 0.94\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 6 > 6.50 AND 8 > 0.50 AND 10 <= 1.50 AND 11 <= 2.50 AND 4 > 0.50 AND 3 <= 2.50\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.02\n",
    "# entropy\n",
    "# Anchor: 5 <= 0.50 AND 10 <= 0.50\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.00\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 5 <= 1.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.32\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 1 <= 1.00 AND 10 <= 1.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.00\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 1 <= 2.00 AND 10 <= 1.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.00\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 1.00 < 7 <= 3.00 AND 0 <= 3.94 AND 1.00 < 5 <= 1.00 AND 6 <= 1.00 AND 8 <= 1.00 AND 12 <= 20.85 AND 2.11 < 4 <= 3.83 AND 2 <= 2382.11 AND 2.00 < 3 <= 3.00 AND 10 <= 40.78 AND 13 <= 0.00 AND 11 <= 18.57\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.00\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 5 <= 1.00 AND 10 <= 1.00\n",
    "# Precision: 0.98\n",
    "# Coverage: 0.00\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 5 <= 0.00 AND 7 > 2.00\n",
    "# Precision: 1.00\n",
    "# Coverage: 0.19\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 7 > 2.00\n",
    "# Precision: 0.96\n",
    "# Coverage: 0.30\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 6 > 4.08 AND 8 > 1.00 AND 10 <= 5.88 AND 3 > 1.00 AND 11 <= 3.00\n",
    "# Precision: 0.95\n",
    "# Coverage: 0.05\n",
    "# GaussianMixtureModels\n",
    "# Anchor: 7 > 2.00 AND 5 <= 0.00\n",
    "# Precision: 0.99\n",
    "# Coverage: 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-architect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tests = {'Number': [0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  5,\n",
    "  6,\n",
    "  7,\n",
    "  8,\n",
    "  9,\n",
    "  10,\n",
    "  11,\n",
    "  12,\n",
    "  13,\n",
    "  14,\n",
    "  15,\n",
    "  16,\n",
    "  17,\n",
    "  18,\n",
    "  19,\n",
    "  20,\n",
    "  21,\n",
    "  22,\n",
    "  23,\n",
    "  24,\n",
    "  25,\n",
    "  26,\n",
    "  27,\n",
    "  28,\n",
    "  29,\n",
    "  30,\n",
    "  31,\n",
    "  32,\n",
    "  33,\n",
    "  34,\n",
    "  35,\n",
    "  36,\n",
    "  37,\n",
    "  38,\n",
    "  39,\n",
    "  40,\n",
    "  41,\n",
    "  42,\n",
    "  43,\n",
    "  44,\n",
    "  45,\n",
    "  46,\n",
    "  47,\n",
    "  48,\n",
    "  49,\n",
    "  50,\n",
    "  51,\n",
    "  52,\n",
    "  53,\n",
    "  54,\n",
    "  55,\n",
    "  56,\n",
    "  57,\n",
    "  58,\n",
    "  59],\n",
    " 'Name': ['MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture',\n",
    "  'MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture',\n",
    "  'MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture',\n",
    "  'MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture',\n",
    "  'MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture',\n",
    "  'MiniBatch\\nKMeans',\n",
    "  'Affinity\\nPropagation',\n",
    "  'MeanShift',\n",
    "  'Spectral\\nClustering',\n",
    "  'Ward',\n",
    "  'Agglomerative\\nClustering',\n",
    "  'DBSCAN',\n",
    "  'OPTICS',\n",
    "  'BIRCH',\n",
    "  'Gaussian\\nMixture'],\n",
    " 'quartile mean precision': [0.9741409494291152,\n",
    "  0.9767200901707267,\n",
    "  0.9777231121016463,\n",
    "  0.9754401523105721,\n",
    "  0.9761588803018756,\n",
    "  0.9721913400326241,\n",
    "  0.9737389891878047,\n",
    "  0.9762463037301493,\n",
    "  0.9730966616969932,\n",
    "  0.9756176801725835,\n",
    "  0.9800150327130065,\n",
    "  0.9798452063790105,\n",
    "  0.9824815306394417,\n",
    "  0.9821816019176953,\n",
    "  0.9763242764563544,\n",
    "  0.9804378863580113,\n",
    "  0.9802128832260655,\n",
    "  0.9814459739456329,\n",
    "  0.9774114346971765,\n",
    "  0.9771833474788476,\n",
    "  0.9546619697430984,\n",
    "  0.9513072845326669,\n",
    "  0.9535273436806041,\n",
    "  0.9566246926613116,\n",
    "  0.9511494722811245,\n",
    "  0.9520133315799743,\n",
    "  0.9555835893818593,\n",
    "  0.9552442675304827,\n",
    "  0.9522449314463134,\n",
    "  0.9539457485601555,\n",
    "  0.9601432614856573,\n",
    "  0.9598359227771676,\n",
    "  0.9590528735426732,\n",
    "  0.9613293090036144,\n",
    "  0.9593817501127138,\n",
    "  0.9609813353504809,\n",
    "  0.9590011123074473,\n",
    "  0.9617587757800141,\n",
    "  0.9609325189911889,\n",
    "  0.9596526820120297,\n",
    "  0.9312181904471097,\n",
    "  0.9304696601824612,\n",
    "  0.9310669034353632,\n",
    "  0.9295603511164102,\n",
    "  0.9282056751806935,\n",
    "  0.9285888255026977,\n",
    "  0.9292644965031165,\n",
    "  0.9283528792174368,\n",
    "  0.9292661953915697,\n",
    "  0.9296918699796757,\n",
    "  0.9701401210312725,\n",
    "  0.9725238555317383,\n",
    "  0.9758930494068495,\n",
    "  0.9742712339917426,\n",
    "  0.9741989432902931,\n",
    "  0.972015353216323,\n",
    "  0.9709680835939047,\n",
    "  0.9708920180019738,\n",
    "  0.9706604763927218,\n",
    "  0.9737936389012097],\n",
    " 'quartile mean coverage': [0.14802966666666667,\n",
    "  0.14295533333333332,\n",
    "  0.14854433333333333,\n",
    "  0.14917566666666668,\n",
    "  0.14488466666666666,\n",
    "  0.14341833333333334,\n",
    "  0.13987333333333332,\n",
    "  0.146675,\n",
    "  0.143049,\n",
    "  0.147536,\n",
    "  0.19080166666666667,\n",
    "  0.187856,\n",
    "  0.18973066666666666,\n",
    "  0.18757966666666667,\n",
    "  0.19336566666666666,\n",
    "  0.189676,\n",
    "  0.189426,\n",
    "  0.19131533333333334,\n",
    "  0.190963,\n",
    "  0.18968166666666667,\n",
    "  0.13904833333333333,\n",
    "  0.13674666666666666,\n",
    "  0.13718133333333332,\n",
    "  0.13763066666666668,\n",
    "  0.13931233333333334,\n",
    "  0.13996533333333333,\n",
    "  0.13725433333333334,\n",
    "  0.13499833333333333,\n",
    "  0.13856966666666667,\n",
    "  0.13906166666666667,\n",
    "  0.268519,\n",
    "  0.2765783333333333,\n",
    "  0.28457333333333334,\n",
    "  0.2703413333333333,\n",
    "  0.278354,\n",
    "  0.2798136666666667,\n",
    "  0.28338033333333335,\n",
    "  0.27444466666666667,\n",
    "  0.2656193333333333,\n",
    "  0.28033033333333335,\n",
    "  0.2577756666666667,\n",
    "  0.25606,\n",
    "  0.2658643333333333,\n",
    "  0.27398533333333336,\n",
    "  0.265789,\n",
    "  0.2637526666666667,\n",
    "  0.25461133333333336,\n",
    "  0.260033,\n",
    "  0.26690566666666665,\n",
    "  0.2537623333333333,\n",
    "  0.14515666666666666,\n",
    "  0.14283466666666667,\n",
    "  0.145705,\n",
    "  0.14371433333333333,\n",
    "  0.144925,\n",
    "  0.144486,\n",
    "  0.14582233333333333,\n",
    "  0.14637933333333333,\n",
    "  0.14156866666666668,\n",
    "  0.14545733333333333],\n",
    " 'decile mean precision': [0.9971038920213745,\n",
    "  0.9969264512559615,\n",
    "  0.9976123131843543,\n",
    "  0.9983765691436547,\n",
    "  0.9969299792977103,\n",
    "  0.9968626613003927,\n",
    "  0.9979913169179515,\n",
    "  0.9972337081937507,\n",
    "  0.9973186964445969,\n",
    "  0.9972098471254538,\n",
    "  0.9891328983526628,\n",
    "  0.9868689995644143,\n",
    "  0.9862005218252159,\n",
    "  0.988024197540695,\n",
    "  0.9891068714673376,\n",
    "  0.9876645097447353,\n",
    "  0.9890950653356122,\n",
    "  0.9887155395233698,\n",
    "  0.9880391728582969,\n",
    "  0.9899105727489956,\n",
    "  0.9849668090286532,\n",
    "  0.9847868908286452,\n",
    "  0.9841076308389758,\n",
    "  0.9856464112430593,\n",
    "  0.9842926568109618,\n",
    "  0.9837507034993961,\n",
    "  0.984021278544243,\n",
    "  0.9842305889609478,\n",
    "  0.9831347626984719,\n",
    "  0.985149209587626,\n",
    "  0.9776374095210423,\n",
    "  0.9796282059837201,\n",
    "  0.9801754854551006,\n",
    "  0.9787722665753902,\n",
    "  0.9792607320334579,\n",
    "  0.9801170058505806,\n",
    "  0.9798019501837592,\n",
    "  0.9795537097971566,\n",
    "  0.979296019487271,\n",
    "  0.9786532025731869,\n",
    "  0.965523329028217,\n",
    "  0.964072927057582,\n",
    "  0.9646173939988656,\n",
    "  0.9631738604989383,\n",
    "  0.962400389293132,\n",
    "  0.9617766465833505,\n",
    "  0.9640819712106719,\n",
    "  0.9629375578723254,\n",
    "  0.9625831449399055,\n",
    "  0.9651219782187156,\n",
    "  0.9937823446506471,\n",
    "  0.9936275182996682,\n",
    "  0.9930959724208185,\n",
    "  0.9952118386053295,\n",
    "  0.9937347842151545,\n",
    "  0.9917960821231163,\n",
    "  0.9931740064928757,\n",
    "  0.9928447227485795,\n",
    "  0.9925713714540726,\n",
    "  0.9935938700575011],\n",
    " 'decile mean coverage': [0.12090133333333333,\n",
    "  0.11862866666666667,\n",
    "  0.115515,\n",
    "  0.116382,\n",
    "  0.12008233333333333,\n",
    "  0.12043333333333334,\n",
    "  0.11524466666666666,\n",
    "  0.11993733333333334,\n",
    "  0.11694766666666667,\n",
    "  0.12066733333333333,\n",
    "  0.171382,\n",
    "  0.17408633333333334,\n",
    "  0.166857,\n",
    "  0.17084866666666668,\n",
    "  0.174877,\n",
    "  0.176585,\n",
    "  0.17091433333333333,\n",
    "  0.17379933333333333,\n",
    "  0.17543266666666665,\n",
    "  0.17572633333333335,\n",
    "  0.09063933333333334,\n",
    "  0.08796733333333333,\n",
    "  0.09013666666666667,\n",
    "  0.08900566666666666,\n",
    "  0.08918,\n",
    "  0.089402,\n",
    "  0.09329866666666667,\n",
    "  0.08477,\n",
    "  0.09182166666666666,\n",
    "  0.09366766666666666,\n",
    "  0.20895833333333333,\n",
    "  0.209115,\n",
    "  0.20633166666666666,\n",
    "  0.213535,\n",
    "  0.215402,\n",
    "  0.21611333333333332,\n",
    "  0.20519766666666667,\n",
    "  0.21575666666666665,\n",
    "  0.205628,\n",
    "  0.21117533333333333,\n",
    "  0.195019,\n",
    "  0.19118533333333335,\n",
    "  0.19695933333333335,\n",
    "  0.20262833333333333,\n",
    "  0.19708633333333334,\n",
    "  0.202274,\n",
    "  0.201789,\n",
    "  0.19372666666666666,\n",
    "  0.20281966666666668,\n",
    "  0.18439933333333333,\n",
    "  0.135429,\n",
    "  0.12891466666666668,\n",
    "  0.13456733333333334,\n",
    "  0.13614133333333334,\n",
    "  0.14041533333333334,\n",
    "  0.142954,\n",
    "  0.136128,\n",
    "  0.13992966666666667,\n",
    "  0.13590366666666667,\n",
    "  0.12999833333333333],\n",
    " 'entropy mean precision': [0.5546736943463958,\n",
    "  0.6831924702216334,\n",
    "  0.5742985269900529,\n",
    "  0.6912368579928793,\n",
    "  0.6512612420016354,\n",
    "  0.5577480410327924,\n",
    "  0.7131512827734083,\n",
    "  0.6489683278055686,\n",
    "  0.5564903310737735,\n",
    "  0.5507167964143963,\n",
    "  0.994500264245721,\n",
    "  0.9926637612502115,\n",
    "  0.9919684711022512,\n",
    "  0.9945480780639487,\n",
    "  0.9910000337161982,\n",
    "  0.9960660210619443,\n",
    "  0.9954327486295268,\n",
    "  0.994224991386514,\n",
    "  0.9945352122223873,\n",
    "  0.993219256530145,\n",
    "  0.8121546110299098,\n",
    "  0.9029476449977293,\n",
    "  0.7668657285441015,\n",
    "  0.8057825251326121,\n",
    "  0.8653872990177388,\n",
    "  0.7702029513375409,\n",
    "  0.7378054933178303,\n",
    "  0.7869030565310974,\n",
    "  0.8764068320956414,\n",
    "  0.7451294717396688,\n",
    "  0.6404600665612533,\n",
    "  0.725652894678529,\n",
    "  0.7495322588696872,\n",
    "  0.7622584255157223,\n",
    "  0.6947365243020811,\n",
    "  0.6316359138395822,\n",
    "  0.7476806917716462,\n",
    "  0.7286877036736926,\n",
    "  0.7938951226272613,\n",
    "  0.6397086623623776,\n",
    "  0.7280630824701635,\n",
    "  0.783295843166759,\n",
    "  0.7633091986491408,\n",
    "  0.8561205621282907,\n",
    "  0.6944321359997999,\n",
    "  0.8574966374289497,\n",
    "  0.6699657553830841,\n",
    "  0.767382830494807,\n",
    "  0.6673124250545116,\n",
    "  0.8689231240510596,\n",
    "  0.7599168982715725,\n",
    "  0.7216693765332052,\n",
    "  0.7556427231198227,\n",
    "  0.8703926726774832,\n",
    "  0.7722462155785232,\n",
    "  0.7387832751526346,\n",
    "  0.8448530079994911,\n",
    "  0.7785123573397416,\n",
    "  0.88332832633553,\n",
    "  0.6676585599572642],\n",
    " 'entropy mean coverage': [0.900511,\n",
    "  0.6291453333333333,\n",
    "  0.8742583333333334,\n",
    "  0.6599546666666667,\n",
    "  0.7128513333333333,\n",
    "  0.8971586666666667,\n",
    "  0.608212,\n",
    "  0.7306956666666666,\n",
    "  0.8969493333333334,\n",
    "  0.9176956666666667,\n",
    "  0.21941733333333333,\n",
    "  0.22565,\n",
    "  0.22408866666666666,\n",
    "  0.22039566666666666,\n",
    "  0.21347933333333333,\n",
    "  0.21797666666666668,\n",
    "  0.218654,\n",
    "  0.21922933333333333,\n",
    "  0.214328,\n",
    "  0.22342599999999999,\n",
    "  0.439451,\n",
    "  0.134746,\n",
    "  0.493786,\n",
    "  0.4524666666666667,\n",
    "  0.2961046666666667,\n",
    "  0.5287143333333333,\n",
    "  0.6022206666666666,\n",
    "  0.522322,\n",
    "  0.25161,\n",
    "  0.5304893333333334,\n",
    "  0.726694,\n",
    "  0.5385476666666666,\n",
    "  0.4925773333333333,\n",
    "  0.5321793333333333,\n",
    "  0.5794673333333333,\n",
    "  0.7342536666666667,\n",
    "  0.5293543333333334,\n",
    "  0.5207313333333333,\n",
    "  0.446749,\n",
    "  0.7329873333333333,\n",
    "  0.46442066666666665,\n",
    "  0.41477033333333335,\n",
    "  0.355428,\n",
    "  0.35822966666666667,\n",
    "  0.497267,\n",
    "  0.3541823333333333,\n",
    "  0.5902326666666666,\n",
    "  0.543661,\n",
    "  0.697269,\n",
    "  0.3431693333333333,\n",
    "  0.544869,\n",
    "  0.49822,\n",
    "  0.5393476666666667,\n",
    "  0.29434566666666667,\n",
    "  0.5172746666666667,\n",
    "  0.582148,\n",
    "  0.41345733333333334,\n",
    "  0.383644,\n",
    "  0.28641,\n",
    "  0.6432286666666667],\n",
    " 'GaussianMixtureModels mean precision': [0.9935886070682411,\n",
    "  0.9928387540435627,\n",
    "  0.9965226514788198,\n",
    "  0.9938809236880374,\n",
    "  0.9965782598458853,\n",
    "  0.9974958371615892,\n",
    "  0.9875969617459274,\n",
    "  0.9900191256368499,\n",
    "  0.9946550076480944,\n",
    "  0.9831255541587898,\n",
    "  0.9939118286935877,\n",
    "  0.9940490727739337,\n",
    "  0.993222506488257,\n",
    "  0.9965688874484172,\n",
    "  0.9952805754642278,\n",
    "  0.9951162615910039,\n",
    "  0.9934165481340085,\n",
    "  0.9949670870457992,\n",
    "  0.9964660432274276,\n",
    "  0.9959468275311019,\n",
    "  0.9451014434562333,\n",
    "  0.9472400640342195,\n",
    "  0.9402581681900917,\n",
    "  0.9408101950791198,\n",
    "  0.9487884485735169,\n",
    "  0.9410359328026497,\n",
    "  0.9434298152142117,\n",
    "  0.9447456414901977,\n",
    "  0.9474408063396321,\n",
    "  0.9497587266131877,\n",
    "  0.9370616407658037,\n",
    "  0.9328852354641952,\n",
    "  0.9432587153346891,\n",
    "  0.9419247518425979,\n",
    "  0.9367971773878325,\n",
    "  0.9362345788831263,\n",
    "  0.935664390960946,\n",
    "  0.9404365403642996,\n",
    "  0.9442011624226416,\n",
    "  0.9337176279832603,\n",
    "  0.9751934811181174,\n",
    "  0.9759996314292451,\n",
    "  0.9748039683789954,\n",
    "  0.9742125904455882,\n",
    "  0.9757450800403673,\n",
    "  0.9744635298163659,\n",
    "  0.9747794304328813,\n",
    "  0.9763874959024186,\n",
    "  0.9749489933458917,\n",
    "  0.9748357625965879,\n",
    "  0.9903589875079256,\n",
    "  0.9661112378284836,\n",
    "  0.988464481535748,\n",
    "  0.9755763542085504,\n",
    "  0.9969054471394488,\n",
    "  0.9836351733974316,\n",
    "  0.9849988026051817,\n",
    "  0.982888307095933,\n",
    "  0.9829832211367694,\n",
    "  0.9584380750612722],\n",
    " 'GaussianMixtureModels mean coverage': [0.123265,\n",
    "  0.12238333333333333,\n",
    "  0.10837066666666667,\n",
    "  0.11707466666666666,\n",
    "  0.10728666666666667,\n",
    "  0.12466233333333333,\n",
    "  0.12601366666666666,\n",
    "  0.11928733333333333,\n",
    "  0.11101,\n",
    "  0.141483,\n",
    "  0.168937,\n",
    "  0.15868733333333335,\n",
    "  0.16740233333333335,\n",
    "  0.15273566666666666,\n",
    "  0.16031366666666666,\n",
    "  0.15273466666666666,\n",
    "  0.15848333333333334,\n",
    "  0.170304,\n",
    "  0.15643100000000001,\n",
    "  0.143067,\n",
    "  0.115373,\n",
    "  0.10343966666666667,\n",
    "  0.11871333333333334,\n",
    "  0.112855,\n",
    "  0.099447,\n",
    "  0.119319,\n",
    "  0.113914,\n",
    "  0.10343366666666666,\n",
    "  0.10519,\n",
    "  0.10518033333333333,\n",
    "  0.30507433333333334,\n",
    "  0.30179066666666665,\n",
    "  0.28794533333333333,\n",
    "  0.29890666666666665,\n",
    "  0.29692666666666667,\n",
    "  0.3039,\n",
    "  0.29527733333333334,\n",
    "  0.2990123333333333,\n",
    "  0.2866923333333333,\n",
    "  0.3061333333333333,\n",
    "  0.19163433333333332,\n",
    "  0.20115266666666667,\n",
    "  0.206933,\n",
    "  0.20883433333333334,\n",
    "  0.20554533333333333,\n",
    "  0.202776,\n",
    "  0.20165333333333332,\n",
    "  0.19749,\n",
    "  0.21164133333333335,\n",
    "  0.20194566666666666,\n",
    "  0.162045,\n",
    "  0.18659666666666666,\n",
    "  0.14361333333333334,\n",
    "  0.14352366666666666,\n",
    "  0.14329633333333333,\n",
    "  0.14803033333333335,\n",
    "  0.17669233333333334,\n",
    "  0.14463766666666666,\n",
    "  0.14985966666666667,\n",
    "  0.20342066666666667]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-ceiling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-shipping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-pilot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-cherry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-ready",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dictionary[\"Data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_dictionary[\"Data\"][dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(data, labels, train_size=0.80)\n",
    "blackbox_model = sklearn.ensemble.RandomForestClassifier(n_estimators=20)\n",
    "blackbox_model.fit(train, labels_train)\n",
    "print('Train', sklearn.metrics.accuracy_score(labels_train, blackbox_model.predict(train)))\n",
    "print('Test', sklearn.metrics.accuracy_score(labels_test, blackbox_model.predict(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['red', 'blue', 'green']\n",
    "feature_names = ['x-axis', 'y-axis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_1 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'quartile')\n",
    "\n",
    "explainer_2 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'decile')\n",
    "\n",
    "explainer_3 = AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train,\n",
    "    train_label = labels_train,\n",
    "    discretizer = 'GaussianMixtureModels')\n",
    "\n",
    "# GaussianMixtureModels\n",
    "# quartile, decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 150\n",
    "\n",
    "np.random.seed(1)\n",
    "print('Prediction: ', explainer_3.class_names[int(blackbox_model.predict(test[idx].reshape(1, -1))[0])])\n",
    "\"\"\" cahnge exp = explainer.explain_instance(...) \n",
    "    to AnchorExplanation \"\"\"\n",
    "exp = explainer_3.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "test_precision = []\n",
    "test_coverage = []\n",
    "for idx in range(len(test)):\n",
    "    exp = explainer_1.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)\n",
    "    test_precision.append(exp.precision())\n",
    "    test_coverage.append(exp.coverage())\n",
    "\n",
    "\n",
    "print(\"mean precision\", statistics.mean(test_precision))\n",
    "print(\"mean coverage\", statistics.mean(test_coverage))\n",
    "\n",
    "results[\"quartile mean precision\"].append(statistics.mean(test_precision))\n",
    "results[\"quartile mean coverage\"].append(statistics.mean(test_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_precision = []\n",
    "test_coverage = []\n",
    "for idx in range(len(test)):\n",
    "    exp = explainer_2.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)\n",
    "    test_precision.append(exp.precision())\n",
    "    test_coverage.append(exp.coverage())\n",
    "\n",
    "\n",
    "print(\"mean precision\", statistics.mean(test_precision))\n",
    "print(\"mean coverage\", statistics.mean(test_coverage))\n",
    "\n",
    "results[\"decile mean precision\"].append(statistics.mean(test_precision))\n",
    "results[\"decile mean coverage\"].append(statistics.mean(test_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_precision = []\n",
    "test_coverage = []\n",
    "print(\"Number of tests\", len(test))\n",
    "for idx in range(len(test)):\n",
    "    if idx % 10 == 0 : print(idx)\n",
    "    exp = explainer_3.explain_instance(test[idx], blackbox_model.predict, threshold=0.95)\n",
    "    test_precision.append(exp.precision())\n",
    "    test_coverage.append(exp.coverage())\n",
    "\n",
    "\n",
    "print(\"mean precision\", statistics.mean(test_precision))\n",
    "print(\"mean coverage\", statistics.mean(test_coverage))\n",
    "\n",
    "results[\"GaussianMixtureModels mean precision\"].append(statistics.mean(test_precision))\n",
    "results[\"GaussianMixtureModels mean coverage\"].append(statistics.mean(test_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Number\"].append(dataset)\n",
    "results[\"Name\"].append(data_dictionary[\"Name\"][dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-detroit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianMixtureModels\n",
    "# quartile, decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-struggle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-neutral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-timber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reflected-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lime.discretize import BaseDiscretizer\n",
    "# class GaussianMixtureModelsDiscretizer(BaseDiscretizer):\n",
    "    \n",
    "#     def __init__(self, train, categorical_features, feature_names, labels, random_state=None, data_stats=None):\n",
    "#         self.train = train\n",
    "#         self.labels = labels\n",
    "        \n",
    "#         if(labels is None):\n",
    "#             raise ValueError('Labels must be not None when using \\\n",
    "#                              GaussianMixtureModelsDiscretizer')\n",
    "            \n",
    "#         BaseDiscretizer.__init__(self, train, categorical_features,\n",
    "#                                  feature_names, labels=labels,\n",
    "#                                  random_state=random_state,\n",
    "#                                  data_stats=data_stats)\n",
    "\n",
    "    \n",
    "#     def getLabels(self):\n",
    "#         train_label = []\n",
    "#         for lebel_number in set(self.labels):\n",
    "#             Classification = np.array([self.train[i] for i in range(len(self.labels)) if self.labels[i] == lebel_number])\n",
    "#             train_label.append(Classification)\n",
    "        \n",
    "#         return train_label\n",
    "    \n",
    "#     # BIC_range = make it an input??\n",
    "#     def gmmBIC(self, train_label_list, BIC_range = [1, 10]):\n",
    "    \n",
    "#         all_gmm_list = []\n",
    "#         for class_C in train_label_list:\n",
    "#             gmm_list = []\n",
    "#             BIC_list = []\n",
    "#             for x in range(min(BIC_range), max(BIC_range) + 1):\n",
    "#                 gmm = mixture.GaussianMixture(n_components= x, covariance_type='full').fit(class_C)\n",
    "#                 BIC_list.append(gmm.bic(class_C))\n",
    "\n",
    "#             min_BIC_index = BIC_list.index(min(BIC_list))\n",
    "#             number_components = min_BIC_index + 1\n",
    "#             gmm = mixture.GaussianMixture(n_components=number_components, covariance_type='full').fit(class_C)\n",
    "#             gmm_list.append(gmm)\n",
    "\n",
    "#             all_gmm_list.append(gmm_list[0])\n",
    "\n",
    "#         return all_gmm_list\n",
    "    \n",
    "#     def getIntervalsGmm(self, gmm, std_constant=3):\n",
    "    \n",
    "#         width_interval_list = []\n",
    "#         height_interval_list = []\n",
    "#         pos_list = []\n",
    "#         angle_list = []\n",
    "\n",
    "#         # CHECK IF THIS WORKS FOR MORE THAN 2D, OTHERWISE CHANGE IT.\n",
    "#         for pos, covariance, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "\n",
    "#             if covariance.shape == (2, 2):\n",
    "#                 U, s, Vt = np.linalg.svd(covariance)\n",
    "#                 angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "#                 width, height = std_constant * np.sqrt(s)\n",
    "#             else:\n",
    "#                 angle = 0\n",
    "#                 width, height = std_constant * np.sqrt(covariance)\n",
    "\n",
    "\n",
    "\n",
    "#             width_interval_list.append(width)\n",
    "#             height_interval_list.append(height)\n",
    "#             pos_list.append(pos)\n",
    "#             angle_list.append(angle)\n",
    "\n",
    "#         return pos_list, width_interval_list, height_interval_list, angle_list\n",
    "    \n",
    "    \n",
    "#     def ellipseBoundingBox(self, pos_list, width_interval_list, height_interval_list, angle_list):\n",
    "    \n",
    "#         x_intervals = []\n",
    "#         y_intervals = []\n",
    "#         for n in range(len(pos_list)):\n",
    "\n",
    "#             h = pos_list[n][0]\n",
    "#             k = pos_list[n][1]\n",
    "#             a = width_interval_list[n]\n",
    "#             b = height_interval_list[n]\n",
    "#             p = np.radians(angle_list[n])\n",
    "\n",
    "#             x_1_angle = np.arctan(-b*np.tan(p)/a)\n",
    "#             x_2_angle = np.arctan(-b*np.tan(p)/a) + math.pi\n",
    "\n",
    "#             y_1_angle = np.arctan(b/(a*np.tan(p)))\n",
    "#             y_2_angle = np.arctan(b/(a*np.tan(p))) + math.pi\n",
    "\n",
    "#             x_1 = h + a*np.cos(x_1_angle)*np.cos(p) - b*np.sin(x_1_angle)*np.sin(p)\n",
    "#             x_2 = h + a*np.cos(x_2_angle)*np.cos(p) - b*np.sin(x_2_angle)*np.sin(p)\n",
    "\n",
    "#             y_1 = k + b*np.sin(y_1_angle)*np.cos(p) + a*np.cos(y_1_angle)*np.sin(p)\n",
    "#             y_2 = k + b*np.sin(y_2_angle)*np.cos(p) + a*np.cos(y_2_angle)*np.sin(p)\n",
    "\n",
    "#             x_intervals.append([x_1, x_2])\n",
    "#             y_intervals.append([y_1, y_2])\n",
    "\n",
    "#         return x_intervals, y_intervals\n",
    "     \n",
    "    \n",
    "#     # make this for more than 2-d\n",
    "#     def getBoundingBoxesIntervals(self, list_gmm):\n",
    "    \n",
    "#         x_intervals_list = []\n",
    "#         y_intervals_list = []\n",
    "#         for gmm in list_gmm:\n",
    "#             pos_list, width_interval_list, height_interval_list, angle_list = self.getIntervalsGmm(gmm)\n",
    "#             x_intervals, y_intervals = self.ellipseBoundingBox(pos_list, width_interval_list, height_interval_list, angle_list)\n",
    "            \n",
    "#             x_flat_intervals = [interval for intervals in x_intervals for interval in intervals]\n",
    "#             y_flat_intervals = [interval for intervals in y_intervals for interval in intervals]\n",
    "#             x_intervals_list.append(x_flat_intervals)\n",
    "#             y_intervals_list.append(y_flat_intervals)\n",
    "            \n",
    "        \n",
    "#         x_flat_intervals_list = [interval for intervals in x_intervals_list for interval in intervals]\n",
    "#         y_flat_intervals_list = [interval for intervals in y_intervals_list for interval in intervals]\n",
    "#         x_flat_intervals_list.sort()\n",
    "#         y_flat_intervals_list.sort()\n",
    "\n",
    "#         feature_intervals_lists = [np.array(x_flat_intervals_list), np.array(y_flat_intervals_list)]\n",
    "        \n",
    "#         return feature_intervals_lists\n",
    "\n",
    "    \n",
    "       \n",
    "        \n",
    "#     def bins(self, data, labels):\n",
    "        \n",
    "#         train_label = self.getLabels()\n",
    "#         all_gmm_list = self.gmmBIC(train_label)\n",
    "#         feature_intervals_lists = self.getBoundingBoxesIntervals(all_gmm_list)\n",
    "        \n",
    "#         bins = feature_intervals_lists\n",
    "        \n",
    "#         return bins\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-benefit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Number\" : [], \"Name\": [], \"quartile mean precision\": [], \"quartile mean coverage\": [], \"decile mean precision\": [], \"decile mean coverage\": [], \"entropy mean precision\": [], \"entropy mean coverage\": [], \"GaussianMixtureModels mean precision\": [], \"GaussianMixtureModels mean coverage\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-plastic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
